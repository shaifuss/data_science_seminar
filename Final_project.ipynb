{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Vn1s8eQR5J",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Online Review Ratings of Pizzarias Using Review Photos \n",
        "\n",
        "Ziv Branstein 301782215\n",
        "\n",
        "Avishai Fuss 332658608"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px_4ubGwSHmx",
        "colab_type": "text"
      },
      "source": [
        "## Scraping\n",
        "As discussed in the project proposal Yelp's dataset does not include photos of reviews, so we had to scrape the photos for our project.\n",
        "For the project proposal we used a basic web scraper, after analyzing the web traffic we used the same requests as their website to download the photos of reviews found in the dataset. This worked well, however, after a few hundred reviews scraped we encountered a consistent \"503 service unavailable\" response, checking their website revealed that we were blocked:\n",
        "![Yelp's ip block](https://github.com/shaifuss/data_science_seminar/blob/master/yelp-ip-address-ban.jpg?raw=1)\n",
        "After disconnecting from the internet and acquiring a new IP address the website was working again, meaning the blocking was done based on the IP.\n",
        "This was a real problem, as there are 479792 reviews in total, and using only a few hundred photos for the project will not produce meaningful results.\n",
        "After getting blocked again we tested the mobile application and it was still operating, giving hope for a new direction for scraping.\n",
        "Analyzing the traffic for the mobile application was a bit more cumbersome - Yelp's app uses SSL pinning(a technique which prevents man-in-the-middle attack for reading the traffic) that needed to be disabled. Also, the HTTP requests performed by the app were signed using logic run inside the app which needed to be reverse engineered.\n",
        "The signature algorithm of the android app uses HMAC-SHA1 on the query string with a key embedded in the binary.\n",
        "\n",
        "The scraping process is performed as follows: \n",
        "```\n",
        "review_scaper.py -> image_scraper.py -> pizza_classifier_xception.py\n",
        "review_scraper.py: For each review it scrapes the urls of the images of the review.\n",
        "image_scraper.py: Given the scraped urls it downloads the photos\n",
        "pizza_classifier_xception.py: Deletes photos which are not classified as pizza in the top 5 results for the Xception classifier\n",
        "```\n",
        "After reverse engineering the signature mechanism we retested the scraping and managed to scrape all 1 star reviews from the dataset and a similar amount of 5 star reviews.\n",
        "After filtering for images of pizzas only (using the pretrained network Xception) we are left with 14,566 photos of pizzas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdXgQf9EUA_n",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA4QlKGA8YTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "from collections import defaultdict\n",
        "from IPython.display import Image\n",
        "from scipy.stats import binom\n",
        "from shutil import copyfile, move\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from traceback import format_exc\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import random\n",
        "import seaborn as sns \n",
        "import tensorflow\n",
        "import tensorflow.keras.applications\n",
        "import time\n",
        "\n",
        "!pip install pyspellchecker -q\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "# Fix random for consistent results\n",
        "def fix_random():\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    tensorflow.random.set_seed(42)\n",
        "fix_random()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4I-ydg8e9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('review_photos'):\n",
        "    if os.path.exists('data_science_seminar'):\n",
        "        %cd data_science_seminar\n",
        "    else:\n",
        "        !git clone https://github.com/shaifuss/data_science_seminar.git\n",
        "        %cd data_science_seminar\n",
        "if (not os.path.exists('yelp_academic_dataset_business.json')) or (not os.path.exists('yelp_academic_dataset_review.json')):\n",
        "    kaggle_path = os.path.expanduser('~/.kaggle')\n",
        "    kaggle_json_path = os.path.join(kaggle_path, 'kaggle.json')\n",
        "    if not os.path.exists(kaggle_json_path):\n",
        "        from getpass import getpass\n",
        "        kaggle_json = getpass('Insert kaggle.json:')\n",
        "        os.makedirs(kaggle_path, exist_ok=True)\n",
        "        with open(kaggle_json_path, 'w') as f:\n",
        "            f.write(kaggle_json)\n",
        "        os.chmod(kaggle_json_path, 0o600)\n",
        "    !kaggle datasets download yelp-dataset/yelp-dataset\n",
        "    !unzip yelp-dataset.zip yelp_academic_dataset_business.json yelp_academic_dataset_review.json\n",
        "    !rm yelp-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfjKh1V8imk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "business_df = pd.read_json('yelp_academic_dataset_business.json', lines=True)\n",
        "print(\"The dataset contains a total of {} businesses\".format(len(business_df.index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yPZKfSk-QA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "business_df = business_df[business_df['categories'].notna()]\n",
        "pizza_biz_df = business_df[business_df['categories'].str.contains(\"Pizza\")]\n",
        "print(\"Of those, {} businesses sell pizza\".format(len(pizza_biz_df.index)))\n",
        "print(\"Here are a few examples\")\n",
        "pizza_biz_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBuz-3W38nq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pizza_business_ids = set(pizza_biz_df.business_id)\n",
        "pizza_reviews = []\n",
        "with open('yelp_academic_dataset_review.json', 'r') as f:\n",
        "    for line in f:\n",
        "        line_json = json.loads(line)\n",
        "        if line_json['business_id'] in pizza_business_ids:\n",
        "            pizza_reviews.append(line_json)\n",
        "print(f\"And a total of {len(pizza_reviews)} reviews\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsioQyhf0hCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_ids_with_photos = list(os.listdir(\"review_photos\"))\n",
        "pizza_reviews_by_id = {pizza_review[\"review_id\"]: pizza_review for pizza_review in pizza_reviews}\n",
        "output_path = \"review_photos_by_stars\"\n",
        "os.makedirs(os.path.join(output_path, \"0\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_path, \"1\"), exist_ok=True)\n",
        "for review_id_with_photos in review_ids_with_photos:\n",
        "    review_path = os.path.join(\"review_photos\", review_id_with_photos)\n",
        "    if pizza_reviews_by_id[review_id_with_photos][\"stars\"] in [1.0, 5.0]:\n",
        "        for filename in os.listdir(review_path):\n",
        "            img_path = os.path.join(review_path, filename)\n",
        "            copyfile(img_path, os.path.join(output_path, str((int(pizza_reviews_by_id[review_id_with_photos][\"stars\"]) - 1) // 4), f\"{review_id_with_photos}_{filename}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msdnhjq3_pcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHSto-oN8e49",
        "colab_type": "text"
      },
      "source": [
        "We are left wtih 4,320 photos from 1-star reviews and 10,246 photos of 5 star reviews. To make training and evaluation easier, we will drop the excess of 5 star review photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ2lJ56V_5jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(max(0, len(os.listdir(os.path.join(output_path, \"1\"))) - len(os.listdir(os.path.join(output_path, \"0\"))))):\n",
        "    os.unlink(os.path.join(output_path, \"1\", np.random.choice(os.listdir(os.path.join(output_path, \"1\")))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33gY_hqUAfvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ET1G_Dy09R",
        "colab_type": "text"
      },
      "source": [
        "We will split the data for training, validation and test sets with 60%:20%:20% respectively. Unlike the proposal, the dataset no longer fits the RAM so we will prepare directories for each subset and feed the model from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYPpGFZJ_EG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_validation_test(input_path, output_path, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    subsets = [\"train\", \"val\", \"test\"]\n",
        "    train_samples = 0\n",
        "    validation_samples = 0\n",
        "    test_samples = 0\n",
        "    for subset in subsets:\n",
        "        os.makedirs(os.path.join(output_path, subset, \"0\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_path, subset, \"1\"), exist_ok=True)\n",
        "    for class_name in os.listdir(input_path):\n",
        "        class_samples = os.listdir(os.path.join(input_path, class_name))\n",
        "        np.random.shuffle(class_samples)\n",
        "        for class_sample in class_samples[:int(len(class_samples) * 0.6)]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"train\", class_name, class_sample))\n",
        "            train_samples += 1\n",
        "        for class_sample in class_samples[int(len(class_samples) * 0.6):int(len(class_samples) * 0.8)]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"val\", class_name, class_sample))\n",
        "            validation_samples += 1\n",
        "        for class_sample in class_samples[int(len(class_samples) * 0.8):]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"test\", class_name, class_sample))\n",
        "            test_samples += 1\n",
        "    print(f\"Splitted data into [training={train_samples}, validation={validation_samples}, test={test_samples}]\")\n",
        "        \n",
        "split_train_validation_test(\"review_photos_by_stars\", \"review_photos_split\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOHOREZHSQ7_",
        "colab_type": "text"
      },
      "source": [
        "We will now try to compare different available pretrained models on our validation set. We will assume the best pretrained model will perform well with other architectures as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgSDJivT83hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "DEFAULT_INPUT_SIZE = 500\n",
        "pretrained_networks =  [\n",
        "  ('DenseNet121', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('DenseNet169', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('DenseNet201', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('InceptionResNetV2', tensorflow.keras.applications.inception_resnet_v2.preprocess_input),\n",
        "  ('InceptionV3', tensorflow.keras.applications.inception_v3.preprocess_input),\n",
        "  ('MobileNet', tensorflow.keras.applications.mobilenet.preprocess_input),\n",
        "  ('MobileNetV2', tensorflow.keras.applications.mobilenet_v2.preprocess_input),\n",
        "  ('NASNetLarge', tensorflow.keras.applications.nasnet.preprocess_input),\n",
        "  ('NASNetMobile', tensorflow.keras.applications.nasnet.preprocess_input),\n",
        "  ('ResNet101', tensorflow.keras.applications.resnet.preprocess_input),\n",
        "  ('ResNet101V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('ResNet152', tensorflow.keras.applications.resnet.preprocess_input),\n",
        "  ('ResNet152V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('ResNet50', tensorflow.keras.applications.resnet50.preprocess_input),\n",
        "  ('ResNet50V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('VGG16', tensorflow.keras.applications.vgg16.preprocess_input),\n",
        "  ('VGG19', tensorflow.keras.applications.vgg19.preprocess_input),\n",
        "  ('Xception', tensorflow.keras.applications.xception.preprocess_input)\n",
        "]\n",
        "patience = 5\n",
        "BATCH_SIZE = 64\n",
        "pretrained_network_results = {}\n",
        "for (pretrained_network_name, pretrained_network_preprocessing) in pretrained_networks:\n",
        "    fix_random()\n",
        "    K.clear_session()\n",
        "    try:\n",
        "        pretrained_network = getattr(tensorflow.keras.applications, pretrained_network_name)\n",
        "        try:\n",
        "            input_shape = (DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE)\n",
        "            base_model = pretrained_network(include_top=False, pooling='avg', input_shape=(input_shape[0], input_shape[1], 3))\n",
        "        except Exception:\n",
        "            # Maybe the model has a specific input shape\n",
        "            base_model = pretrained_network(include_top=False, pooling='avg')\n",
        "            input_shape = (base_model.input_shape[1], base_model.input_shape[2])\n",
        "        train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "            directory=\"review_photos_split/train\", \n",
        "            class_mode=\"binary\", \n",
        "            target_size=input_shape, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=False\n",
        "        )\n",
        "        X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "        y_train = train_generator.classes\n",
        "        validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "            directory=\"review_photos_split/val\", \n",
        "            class_mode=\"binary\", \n",
        "            target_size=input_shape, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=False\n",
        "        )\n",
        "        X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "        y_validation = validation_generator.classes\n",
        "        K.clear_session()\n",
        "        input_layer = Input(shape=(base_model.output_shape[1], ))\n",
        "        x = Dense(8, activation='relu')(input_layer)\n",
        "        predictions = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "        h = model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            validation_data = (X_validation, y_validation), \n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1000, \n",
        "            callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "            verbose=0\n",
        "        )\n",
        "        pretrained_network_results[pretrained_network_name] = h.history['val_accuracy'][-patience - 1]\n",
        "    except Exception:\n",
        "      print(f\"Warning - failed to execute {pretrained_network_name}: {format_exc()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwRusU9Rmy2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "sorted_pretrained_network_results = sorted(pretrained_network_results.items(), key=lambda result_item: result_item[1])\n",
        "barplot = sns.barplot(x=[network_name for (network_name, _) in sorted_pretrained_network_results], y=[network_result for (_, network_result) in sorted_pretrained_network_results])\n",
        "_, pretrained_network_result_values = zip(*sorted_pretrained_network_results)\n",
        "barplot.set(ylim=(min(pretrained_network_result_values) - 0.01, max(pretrained_network_result_values) + 0.01))\n",
        "plt.ylabel(\"Validation Accuracy\", fontsize=14)\n",
        "plt.xlabel(\"Pretrained Network\", fontsize=14)\n",
        "for i, p in enumerate(barplot.patches):\n",
        "    barplot.text(i, p.get_height() + 0.0005, '%.2f%%' % (sorted_pretrained_network_results[i][1] * 100, ), color='black', ha=\"center\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTWgb24iUXOw",
        "colab_type": "text"
      },
      "source": [
        "As you can see DenseNet201 performed the best on our validation set.\n",
        "From now on we will use the extracted features from DenseNet201 as the input for different algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jfTtS5fzGC",
        "colab_type": "text"
      },
      "source": [
        "### Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRPlJLfEVcjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "# Create the base pre-trained model\n",
        "base_model = tensorflow.keras.applications.DenseNet201(include_top=False, input_shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3), pooling='avg')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "test_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/test\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "# Precalculate extracted features for the different sets once to save time\n",
        "X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "y_train = train_generator.classes\n",
        "X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "y_validation = validation_generator.classes\n",
        "X_test = base_model.predict(test_generator, batch_size=BATCH_SIZE)\n",
        "y_test = test_generator.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avgwO25enqbg",
        "colab_type": "text"
      },
      "source": [
        "We will try to use a Dropout layer to avoid overfitting on the training set. \n",
        "\n",
        "We also decided to drop the data augmentation, since it made the training very slow and we didn't see a significant improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neKKxATLfGuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_random()\n",
        "patience = 15\n",
        "BATCH_SIZE = 128\n",
        "input_layer = Input(shape=(base_model.output_shape[1], ))\n",
        "x = Dropout(0.5)(input_layer)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "h = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_validation, y_validation), \n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5c_SHlznKnr",
        "colab_type": "text"
      },
      "source": [
        "We can see an improvement over the original result.\n",
        "\n",
        "Let's try to fine-tune the model to further improve the results. We will recreate the model with the trained weights, unfreeze the last few layers in the base model, and use a low learning rate to avoid destroying the pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd4hIC6dmWoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "input_layer = Input(shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3))\n",
        "x = Dense(8, activation='relu')(base_model(input_layer, training=False))\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "fine_tune_model = Model(inputs=input_layer, outputs=predictions)\n",
        "fine_tune_model.layers[-1].set_weights(model.layers[-1].get_weights())\n",
        "fine_tune_model.layers[-2].set_weights(model.layers[-2].get_weights())\n",
        "for layer in base_model.layers[:-9]:\n",
        "    layer.trainable = False\n",
        "fine_tune_model.compile(optimizer=SGD(learning_rate=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "patience = 3\n",
        "train_generator = ImageDataGenerator(\n",
        "    preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input,\n",
        ").flow_from_directory(\n",
        "    directory=\"review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=42\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=42\n",
        ")\n",
        "h = fine_tune_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucyOayELU9k4",
        "colab_type": "text"
      },
      "source": [
        "### Support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBHfROTU8dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_random()\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_validation_predicted = svm.predict(X_validation)\n",
        "print(f\"Validation accuracy for SVM over pretrained model is: {np.sum(y_validation_predicted == y_validation) / y_validation.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvAdgEQD1M0Z",
        "colab_type": "text"
      },
      "source": [
        "SVM performed better than the neural network on the validation set. Let's see if it has the same accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASXozUgYp1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_predicted = svm.predict(X_test)\n",
        "print(f\"Test accuracy for the SVM over pretrained model is: {np.sum(y_test_predicted == y_test) / y_test.shape[0]}\")\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_predicted), index=['True: 1 star', 'True: 5 stars'], columns=['Predicted: 1 star', 'Predicted: 5 stars'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tth6Sy9gsf6j",
        "colab_type": "text"
      },
      "source": [
        "It seems like the test set accuracy is similar to the validation accuracy which probably indicates that the model has generalized pretty well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kO4tXeNljtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Free memory for the next section\n",
        "del base_model\n",
        "del model\n",
        "del fine_tune_model\n",
        "del svm\n",
        "del X_train\n",
        "del y_train\n",
        "del X_validation\n",
        "del y_validation\n",
        "del X_test\n",
        "del y_test\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYRRLyWXAQai",
        "colab_type": "text"
      },
      "source": [
        "### NLP Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ruz0EdAC4a",
        "colab_type": "text"
      },
      "source": [
        "In this section we aim to identify distinct topics discussed in the corpus of pizza review texts. Once topics are identified, reviews that do not contain food-related topics can be filtered out. This has the potential to improve the image classification in the next stage by weeding out irrelevent samples - where reviewers didn't base their scores on the pizza. \n",
        "\n",
        "This will be achieved by building a topic model that categorizes the information present in each review. A topic can be modeled as a set of words that are all related. For instance, we might say that [noise, smell, music, dirt, lighting] reflects the topic of restaurant atmosphere.  \n",
        "\n",
        "The first algorithm we will utilize is latent Dirichlet allocation. The premise of LDA (Biel et al., 2003) is that documents with similar topics use similar words. The algorithm aims to discover groups of words the occur frequently occur together in the same document. A topic is modeled as a probability distribution over words. Moreover, a document can be modeled as a probability distribution over different topics. \n",
        "\n",
        "Thus, the algorithm works as follows:\n",
        "\n",
        "1.   Remove unimportant words and set how many topics to find.\n",
        "2.   Randomly assign each word in each document to a random topic\n",
        "3.   For each document,\n",
        ">a. choose a topic, assuming all others are allocated correctly\n",
        "\n",
        ">>i. calculate the topic distribution within the document:  p(topic | document)\n",
        "\n",
        ">>ii. calculate the word distribution within the topic: p(word | topic)\n",
        "\n",
        ">> iii. multiply i and ii together and assign words to new topics based on the result\n",
        "\n",
        "4. terminate when there are no new assignments\n",
        "\n",
        "The LDA model is finetuned by several parameters:\n",
        "Alpha reflects how many topics are in a given document (higher values lead to more topics per document in the model)\n",
        "Beta reflects how many words are in a given topic (higher values lead to more words per topic in the model). In this implementation, the model is set to learn these values automatically.\n",
        "\n",
        "We compare the pure LDA model with a hybrid version that utilizes sentence embeddings of the review texts crafted by BERT (Devlin et al., 2018). Inspiration for this hybrid model comes from https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032.\n",
        "\n",
        "BERT is a pretrained language model from researchers at Google that utilizes a multiheaded transformer ANN architecture to craft word vectors that learns to capture the meaning of words from the context in which they are found. BERT and its successors have acheived impressive performance on language understatnding tasks like question answering and others. \n",
        "\n",
        "The hybrid model attempted here fuses the topic probability vector from LDA with the review text embedding from BERT to create a hybrid sample. Since the BERT vectors are much larger that the LDA vectors, scaling is performed on the LDA vectors to even out their relative importance. \n",
        "\n",
        "Clustering is then performed to distinguish different topics in the corpus (the default algorithm used here is KMeans). The most frequent words in the cluster become the topic. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uxtOU-NAdOl",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is necessary to create a universal vocabulary for the corpus. Each text is first processed at the string level and then at the word level. \n",
        "\n",
        "Examples:\n",
        "1. fix typos and missing spaces\n",
        "2. remove punctuation, capitalization, and numbers\n",
        "3. remove unimportant words (stopwords)\n",
        "4. stem words so that a fair comparison can be made (for example, salted and salty both become salt)\n",
        "\n",
        "For the sake of brevity, the preproccessing functions are contained in a seperate script located at:\n",
        "https://github.com/shaifuss/data_science_seminar/blob/master/preproccessing.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K82IjiGhCMo2",
        "colab_type": "text"
      },
      "source": [
        "In an attempt to improve the clustering for the hybrid model, dimensionality reduction is performed. An autoencoder is trained on the LDA+BERT vectors. Once training is complete, the middle hidden layer of length 32 is taken as a representation of the original data. This achieves compression of at least 25x. \n",
        "\n",
        "Clustering is performed on the compressed vectors. After clustering, topics are constructed from the top 5 most frequent words in each cluster.\n",
        "\n",
        "For the hybrid model, BERT outputs sentence-level encodings of length 768 (the first output vector corresponding to the CLS token passed in at the start of each text). An LDA vector contains a probability value in range [0.0, 1.0] for each possible topic. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQzgdG14COCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=200,\n",
        "                                        batch_size=128,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBiHU0lC8vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, method, k=4):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 250  # parameter for reletive importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + str(round(time.time(),0))\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        Get vector representations from selected methods\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'LDA':\n",
        "            print('Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20, alpha='auto', )\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            return vec\n",
        "\n",
        "        #         elif method == 'LDA_BERT':\n",
        "        else: \n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('Fitting Autoencoder ...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        Fit the topic model for selected method given the preprocessed data\n",
        "        :docs: list of documents, each doc is preprocessed as tokens\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('Fitting LDA ...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary\n",
        "                                                                , alpha='auto', eta='auto', minimum_probability=0.3)\n",
        "        else:\n",
        "            print('Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "        \"\"\"\n",
        "        Predict topics for new_documents\n",
        "        \"\"\"\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "\n",
        "        print(\"Predicting...\")\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":   # take the most prevalent topic\n",
        "            #lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     #key=lambda x: x[1], reverse=True)[0][0], corpus)))\n",
        "            lbs = []\n",
        "            for text in corpus:\n",
        "              lbs.append(self.ldamodel.get_document_topics(text))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs\n",
        "    def persist(self, workdir):\n",
        "      with open(os.path.join(workdir, \"test_\" + self.id), 'wb') as f:\n",
        "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjdLrj_5DGvw",
        "colab_type": "text"
      },
      "source": [
        "Coherence is used for topic model evaluation (Roder et al., 2015). As topic models produced algorithmically are sometimes not easily interpreted by humans, it is necessary to have a way to objectively measure how well the words in a topic go together. \n",
        "\n",
        "Very generally, coherence can be evaluated for a topic by first computing a Cartesian product on itself to construct pairs (excluding twin pairs). The co-occurence of each pair is checked in an external reference - a very large corpus of texts said to represent common language usage. The higher the co-occurence scores, the higher the topic coherence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDekpuykvS47",
        "colab_type": "text"
      },
      "source": [
        "We hypothesize that the following four topics are relevant to pizza reviews.\n",
        "\n",
        "1. Food\n",
        "2. Service\n",
        "3. Atmosphere\n",
        "4. Value\n",
        "\n",
        "Adding +/- 1, we train models with 3, 4, and 5 topics for each method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2urPDtaoRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(method, ntopic, sentences, token_lists, idx_in):\n",
        "  \n",
        "  print(\"Starting training...\")\n",
        "  start = time.time()\n",
        "  tm = Topic_Model(method, k = ntopic)\n",
        "  tm.fit(sentences, token_lists)\n",
        "  end = time.time()\n",
        "  print(\"Training on {} reviews took {} minutes\".format(len(sentences), str((end - start)/60)))\n",
        "  \n",
        "  return tm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQQy8uEoa3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessing\n",
        "import postprocess\n",
        "\n",
        "pic_path = 'review_photos'\n",
        "pix_review_ids = [f for f in os.listdir(pic_path) if os.path.isdir(os.path.join(pic_path, f))]\n",
        "\n",
        "text_df = pd.DataFrame.from_records(pizza_reviews)\n",
        "\n",
        "pic_review_df = text_df[text_df['review_id'].isin(pix_review_ids)]\n",
        "pic_review_df = pic_review_df.reset_index(drop=True)\n",
        "pic_review_df = pic_review_df.fillna('')\n",
        "reviews = pic_review_df.text\n",
        "\n",
        "sentences, token_lists, idx_in = preprocessing.preprocess(reviews, len(reviews) + 1)\n",
        "\n",
        "model_dict = dict()\n",
        "for method in [\"LDA\", \"LDA_BERT\"]:\n",
        "  for num_topics in range(3, 6):\n",
        "    tm = train(method, num_topics, sentences, token_lists, idx_in)\n",
        "    model_dict[method + \"_\" + str(num_topics)] = dict()\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"model\"] = tm\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"coherence\"] = postprocess.get_coherence(tm, token_lists, 'c_v')\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"silhouette\"] = postprocess.get_silhouette(tm)\n",
        "    if method == \"LDA\":\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = postprocess.get_topic_words_lda(tm)\n",
        "    else:\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = postprocess.get_topic_words_hybrid(token_lists, tm.cluster_model.labels_, k=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph1S4BbBbGEJ",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "1. Coherence was consistently an order of magnitude higher for the pure LDA model, a gap that widened as larger datasets were used.\n",
        "2. Silhoutte scores improved as the LDA vectors were weighted more heavily in the hybrid model (not shown in this notebook)\n",
        "3. The pure LDA model produced higher quality topics as judged by two human evaluators (us). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR5CIIYosI7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_pizza_vocab = preprocessing.stem_words([\"crust\", \"slice\", \"sauce\", \"topping\", \"cheese\", \"tomato\", \"food\", \"pie\"])\n",
        "model_dict = postprocess.get_scores(model_dict, stemmed_pizza_vocab)\n",
        "best_method = max(model_dict, key=lambda v: model_dict[v]['coherence'])\n",
        "best_topic = max(range(len(model_dict[best_method][\"scores\"])), key=model_dict[best_method][\"scores\"].__getitem__)\n",
        "print(\"best method is {}\".format(best_method))\n",
        "print(\"best_topic is at index {}\".format(best_topic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P02iiJwSQWYI",
        "colab_type": "text"
      },
      "source": [
        "For prediction, the topics are retrieved for a given review. If the best pizza topic is not the most prominent topic in that review, our hypothesis is that discarding it will lead to improved performance for the image classifier. This would be the ideal filter. However, due to constaints on the size of the dataset, we eliminate reviews where the pizza topic is below a minimum threshold instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZNfuv3QVgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "topic_lists = model_dict[best_method][\"model\"].predict(sentences, token_lists, True)\n",
        "indicies = []\n",
        "for i, topic_list in enumerate(topic_lists):\n",
        "  topics = [x[0] for x in topic_list]\n",
        "  if best_topic in topics:\n",
        "    indicies.append(i)\n",
        "\n",
        "food_based_reviews = pic_review_df[pic_review_df.index.isin(indicies)]\n",
        "print(\"Example reviews that passed the filter:\\n\")\n",
        "print(food_based_reviews.loc[random.choice(indicies), \"text\"])\n",
        "print(\"---------------------------------------------------------------\")\n",
        "print(food_based_reviews.loc[random.choice(indicies), \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc1Tug0ED8LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "food_based_review_ids = set(food_based_reviews[\"review_id\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfb0aoo5ERHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_ids_with_photos = list(os.listdir(\"review_photos\"))\n",
        "pizza_reviews_by_id = {pizza_review[\"review_id\"]: pizza_review for pizza_review in pizza_reviews}\n",
        "output_path = \"filtered_review_photos_by_stars\"\n",
        "os.makedirs(os.path.join(output_path, \"0\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_path, \"1\"), exist_ok=True)\n",
        "for review_id_with_photos in review_ids_with_photos:\n",
        "    review_path = os.path.join(\"review_photos\", review_id_with_photos)\n",
        "    if review_id_with_photos not in food_based_review_ids:\n",
        "        # Filter non food based reviews \n",
        "        continue\n",
        "    if pizza_reviews_by_id[review_id_with_photos][\"stars\"] in [1.0, 5.0]:\n",
        "        for filename in os.listdir(review_path):\n",
        "            img_path = os.path.join(review_path, filename)\n",
        "            copyfile(img_path, os.path.join(output_path, str((int(pizza_reviews_by_id[review_id_with_photos][\"stars\"]) - 1) // 4), f\"{review_id_with_photos}_{filename}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "655IJ9ItEvg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPpPg8-E0aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(max(0, len(os.listdir(os.path.join(output_path, \"1\"))) - len(os.listdir(os.path.join(output_path, \"0\"))))):\n",
        "    os.unlink(os.path.join(output_path, \"1\", np.random.choice(os.listdir(os.path.join(output_path, \"1\")))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nsgbC7KE42U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyiJz0piE6fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_train_validation_test(\"filtered_review_photos_by_stars\", \"filtered_review_photos_split\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNHBDKx2FEsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "# Create the base pre-trained model\n",
        "base_model = tensorflow.keras.applications.DenseNet201(include_top=False, input_shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3), pooling='avg')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"filtered_review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"filtered_review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "test_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"filtered_review_photos_split/test\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "# Precalculate extracted features for the different sets once to save time\n",
        "X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "y_train = train_generator.classes\n",
        "X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "y_validation = validation_generator.classes\n",
        "X_test = base_model.predict(test_generator, batch_size=BATCH_SIZE)\n",
        "y_test = test_generator.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwg0t7fiFRRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_random()\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_validation_predicted = svm.predict(X_validation)\n",
        "print(f\"Validation accuracy for SVM over pretrained model is: {np.sum(y_validation_predicted == y_validation) / y_validation.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqoosqqNF53K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_predicted = svm.predict(X_test)\n",
        "print(f\"Test accuracy for the SVM over pretrained model is: {np.sum(y_test_predicted == y_test) / y_test.shape[0]}\")\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_predicted), index=['True: 1 star', 'True: 5 stars'], columns=['Predicted: 1 star', 'Predicted: 5 stars'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fYI4H7s_qNx",
        "colab_type": "text"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRhCNo4A2yfp",
        "colab_type": "text"
      },
      "source": [
        "Review rating prediction (RRP) is the branch of research studying the connection between reviews and ratings. Most commonly, Review texts are used to predict either a multi-class rating classification or regression model. Two main approaches appear in the literature.\n",
        "\n",
        "The first uses review texts alone. Using the same YELP dataset, Ashgar et al. (2016) compared many different NLP methods to perform multi-class classification.\n",
        "\n",
        "The second approach attempts to combine textual inputs with other information available from the review. Wang and colleagues (2016) created a model that combines review texts with author habits. \n",
        "\n",
        "In this project, we go in a different direction. We place the focus on the visual information from the reviews and incorporate the textual information in support. We hypothesize that product images can be the driving factor for gleaning information about ratings. \n",
        "\n",
        "A Yelp restaurant review encompasses the entire dining experience; taste, presentation, portion size, service, atmosphere, quality, price, ambience and more. Other factors more abstract influences include: each reviewer weighs factors differently, outside influences on the reviewer’s mood that day, variations in cultural rating habits, varied levels of photography skills, to name a few. Considering all of this, it’s not a foregone conclusion that a connection between presentation and rating exists. Even if it does, it’s quite a daunting task to find it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt_OSP-c_th_",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNHdIBtC24FH",
        "colab_type": "text"
      },
      "source": [
        "We achieved over 75% accuracy both on the validation and test sets(compared to the 69% on the much smaller dataset in the proposal) with image classification alone. The matching results for both sets indicate that the model has indeed learned a generalizable connection between images and ratings.\n",
        "Even so, there's room for improvement, one reason being that our resources were limited (e.g. we omitted data augmentation to fit the data in the given execution limit).\n",
        "\n",
        "The dataset contains nearly 480,000 texts. Ideally we would have liked to use them all to build the models but the computational costs became prohibitive. As mentioned above, scraping images was a significant obstacle in curating this self-made dataset.\n",
        "\n",
        "While building the NLP model, we used ~60,000 reviews which we preprocessed over a 12 hour period and persisted to disk. For the final version, we were required to run the notebook consecutively from beginning to end. Given the constraints of the environment (Google Colab), were only able to use the 10304 reviews for which we scraped images.\n",
        "\n",
        "The impact of the relatively small dataset was most evident when choosing the filtering condition. We originally planned on retaining only reviews whose top-rated topic was the pizza topic. However, this filtered out 90% of the data. Instead, we filtered out the reviews where the pizza topic didn’t pass a minimum threshold. Obviously, this condition is much less strict and served to hamper the potential of NLP filtering. \n",
        "\n",
        "In both the image classification portion and the NLP portion, a modern neural network approach is compared with a more “traditional” algorithm. Surprisingly, both times the traditional algorithm performed best. For the NLP, this may be because the LDA Bayesian-based algorithm was specifically crafted for the task while the clustering hybrid model may have been learning other aspects inherent in the data. \n",
        "\n",
        "As presently constructed, an experience with 20 photos carried more influence than a review with 1 photo. This is because each photo was treated as a separate sample. Further work should consider multiple instance learning to control for this.\n",
        "\n",
        "The model has to learn to discount reviews that put little weight on appearance – a tendency of the review author. Hypothetically, multiple instance learning could also help with this.\n"
      ]
    }
  ]
}