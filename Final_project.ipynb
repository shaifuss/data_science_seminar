{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da23aeba3c1048c99a1b9e6ed2d270d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03dc14370b3343749733fe9faa727cd5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bdaa02fb0be34833b4881051f954632e",
              "IPY_MODEL_5a912acb496847e9b395509ebbce9ace"
            ]
          }
        },
        "03dc14370b3343749733fe9faa727cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdaa02fb0be34833b4881051f954632e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d30078b9dea404a929f028e04dd8d91",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efa294928125490f97901b40b033bac2"
          }
        },
        "5a912acb496847e9b395509ebbce9ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6da70117fa57481db5785f89c0a29734",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  6.24it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71737015f401430d8cdf69f9fe120a49"
          }
        },
        "4d30078b9dea404a929f028e04dd8d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efa294928125490f97901b40b033bac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6da70117fa57481db5785f89c0a29734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71737015f401430d8cdf69f9fe120a49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f3923a126064938b04f7aaf3a668e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2dc138e3b7a94e20b92bd731562a5685",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b9c68ab143fe4cd483b5bf022e27ace9",
              "IPY_MODEL_f40f15854ca34799bca81593e18025f1"
            ]
          }
        },
        "2dc138e3b7a94e20b92bd731562a5685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9c68ab143fe4cd483b5bf022e27ace9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_be930335918e4db7b3158aebcf599f23",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c02f8da18a8943559cff512243ccbcdf"
          }
        },
        "f40f15854ca34799bca81593e18025f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3adcf86845a14d4fa6f0915d7c25ad13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  5.99it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87c77ad7ea914d6aaf9dc94e2a4af940"
          }
        },
        "be930335918e4db7b3158aebcf599f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c02f8da18a8943559cff512243ccbcdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3adcf86845a14d4fa6f0915d7c25ad13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87c77ad7ea914d6aaf9dc94e2a4af940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7f6cdbd4c5347878cdc0b499676f0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c14264015d944938eb93eefa236e366",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8650b1f3842e49f0809ae4448b3d145b",
              "IPY_MODEL_af570ed098a742c999a4790a144138ff"
            ]
          }
        },
        "7c14264015d944938eb93eefa236e366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8650b1f3842e49f0809ae4448b3d145b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b0ed14644a7d4aa9b671e409ea67058e",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47ca6483e8874c729a50cf37ba4f3f14"
          }
        },
        "af570ed098a742c999a4790a144138ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e9fa70ad085d420f87241ae6c350a4d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  6.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1bfae2491004246b1c07e839f67ae0a"
          }
        },
        "b0ed14644a7d4aa9b671e409ea67058e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47ca6483e8874c729a50cf37ba4f3f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9fa70ad085d420f87241ae6c350a4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1bfae2491004246b1c07e839f67ae0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Vn1s8eQR5J",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Online Review Ratings of Pizzarias Using Review Photos \n",
        "\n",
        "Ziv Branstein 301782215\n",
        "\n",
        "Avishai Fuss 332658608"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px_4ubGwSHmx",
        "colab_type": "text"
      },
      "source": [
        "## Scraping\n",
        "As discussed in the project proposal Yelp's dataset does not include photos of reviews, so we had to scrape the photos for our project.\n",
        "For the project proposal we used a basic web scraper, after analyzing the web traffic we used the same requests as their website to download the photos of reviews found in the dataset. This worked well, however, after a few hundred reviews scraped we encountered a consistent \"503 service unavailable\" response, checking their website revealed that we were blocked:\n",
        "![Yelp's ip block](https://github.com/shaifuss/data_science_seminar/blob/master/yelp-ip-address-ban.jpg?raw=1)\n",
        "After disconnecting from the internet and acquiring a new IP address the website was working again, meaning the blocking was done based on the IP.\n",
        "This was a real problem, as there are 479792 reviews in total, and using only a few hundred photos for the project will not produce meaningful results.\n",
        "After getting blocked again we tested the mobile application and it was still operating, giving hope for a new direction for scraping.\n",
        "Analyzing the traffic for the mobile application was a bit more cumbersome - Yelp's app uses SSL pinning(a technique which prevents man-in-the-middle attack for reading the traffic) that needed to be disabled. Also, the HTTP requests performed by the app were signed using logic run inside the app which needed to be reverse engineered.\n",
        "The signature algorithm of the android app uses HMAC-SHA1 on the query string with a key embedded in the binary.\n",
        "\n",
        "The scraping process is performed as follows: \n",
        "```\n",
        "review_scaper.py -> image_scraper.py -> pizza_classifier_xception.py\n",
        "review_scraper.py: For each review it scrapes the urls of the images of the review.\n",
        "image_scraper.py: Given the scraped urls it downloads the photos\n",
        "pizza_classifier_xception.py: Deletes photos which are not classified as pizza in the top 5 results for the Xception classifier\n",
        "```\n",
        "After reverse engineering the signature mechanism we retested the scraping and managed to scrape all 1 star reviews from the dataset and a similar amount of 5 star reviews.\n",
        "After filtering for images of pizzas only (using the pretrained network Xception) we are left with 14,566 photos of pizzas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdXgQf9EUA_n",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA4QlKGA8YTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "from collections import defaultdict\n",
        "from IPython.display import Image\n",
        "from scipy.stats import binom\n",
        "from shutil import copyfile, move\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from traceback import format_exc\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import random\n",
        "import seaborn as sns \n",
        "import tensorflow\n",
        "import tensorflow.keras.applications\n",
        "import time\n",
        "\n",
        "!pip install pyspellchecker -q\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "# Fix random for consistent results\n",
        "def fix_random():\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    tensorflow.random.set_seed(42)\n",
        "fix_random()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZyJJd5PFSE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0653ed0f-9f6b-4b04-98bc-c5f7f469bd4e"
      },
      "source": [
        "%cd ..\n",
        "%rm -rf data_science_seminar/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4I-ydg8e9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "55630257-9e51-426e-cad7-6a56f651cd8f"
      },
      "source": [
        "if not os.path.exists('review_photos'):\n",
        "    if os.path.exists('data_science_seminar'):\n",
        "        %cd data_science_seminar\n",
        "    else:\n",
        "        !git clone https://github.com/shaifuss/data_science_seminar.git\n",
        "        %cd data_science_seminar\n",
        "if (not os.path.exists('yelp_academic_dataset_business.json')) or (not os.path.exists('yelp_academic_dataset_review.json')):\n",
        "    kaggle_path = os.path.expanduser('~/.kaggle')\n",
        "    kaggle_json_path = os.path.join(kaggle_path, 'kaggle.json')\n",
        "    if not os.path.exists(kaggle_json_path):\n",
        "        from getpass import getpass\n",
        "        kaggle_json = getpass('Insert kaggle.json:')\n",
        "        os.makedirs(kaggle_path, exist_ok=True)\n",
        "        with open(kaggle_json_path, 'w') as f:\n",
        "            f.write(kaggle_json)\n",
        "        os.chmod(kaggle_json_path, 0o600)\n",
        "    !kaggle datasets download yelp-dataset/yelp-dataset\n",
        "    !unzip yelp-dataset.zip yelp_academic_dataset_business.json yelp_academic_dataset_review.json\n",
        "    !rm yelp-dataset.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'data_science_seminar'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 23725 (delta 45), reused 7 (delta 1), pack-reused 23643\u001b[K\n",
            "Receiving objects: 100% (23725/23725), 1.66 GiB | 16.23 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n",
            "Checking out files: 100% (14967/14967), done.\n",
            "/data_science_seminar\n",
            "Downloading yelp-dataset.zip to /data_science_seminar\n",
            "100% 4.48G/4.48G [01:58<00:00, 34.5MB/s]\n",
            "100% 4.48G/4.48G [01:58<00:00, 40.7MB/s]\n",
            "Archive:  yelp-dataset.zip\n",
            "  inflating: yelp_academic_dataset_business.json  \n",
            "  inflating: yelp_academic_dataset_review.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfjKh1V8imk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcadc674-2e83-4b02-a928-4cf1323a4e2f"
      },
      "source": [
        "business_df = pd.read_json('yelp_academic_dataset_business.json', lines=True)\n",
        "print(\"The dataset contains a total of {} businesses\".format(len(business_df.index)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset contains a total of 209393 businesses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yPZKfSk-QA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "f956c7ea-7eaf-4962-dd75-0427849e1c95"
      },
      "source": [
        "business_df = business_df[business_df['categories'].notna()]\n",
        "pizza_biz_df = business_df[business_df['categories'].str.contains(\"Pizza\")]\n",
        "print(\"Of those, {} businesses sell pizza\".format(len(pizza_biz_df.index)))\n",
        "print(\"Here are a few examples\")\n",
        "pizza_biz_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Of those, 7302 businesses sell pizza\n",
            "Here are a few examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>name</th>\n",
              "      <th>address</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>postal_code</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>is_open</th>\n",
              "      <th>attributes</th>\n",
              "      <th>categories</th>\n",
              "      <th>hours</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>ZkzutF0P_u0C0yTulwaHkA</td>\n",
              "      <td>Lelulos Pizzeria</td>\n",
              "      <td>311 Unity Center Rd</td>\n",
              "      <td>Plum</td>\n",
              "      <td>PA</td>\n",
              "      <td>15239</td>\n",
              "      <td>40.489996</td>\n",
              "      <td>-79.779288</td>\n",
              "      <td>4.0</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>{'RestaurantsPriceRange2': '1', 'BusinessAccep...</td>\n",
              "      <td>Restaurants, Pizza</td>\n",
              "      <td>{'Monday': '0:0-0:0', 'Tuesday': '11:0-21:0', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>OWkS1FXNJbozn-qPg3LWxg</td>\n",
              "      <td>Mama Napoli Pizza</td>\n",
              "      <td></td>\n",
              "      <td>Las Vegas</td>\n",
              "      <td>NV</td>\n",
              "      <td>89109</td>\n",
              "      <td>36.128561</td>\n",
              "      <td>-115.171130</td>\n",
              "      <td>4.5</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>{'RestaurantsDelivery': 'False', 'BusinessAcce...</td>\n",
              "      <td>Food, Food Trucks, Restaurants, Pizza</td>\n",
              "      <td>{'Friday': '18:0-0:0'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-C0AlwLuXpcP609madJZQQ</td>\n",
              "      <td>Pizzaville</td>\n",
              "      <td>1030 Kennedy Circle, Unit 10</td>\n",
              "      <td>Milton</td>\n",
              "      <td>ON</td>\n",
              "      <td>L9T 0J9</td>\n",
              "      <td>43.508962</td>\n",
              "      <td>-79.837990</td>\n",
              "      <td>3.5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>Restaurants, Pizza</td>\n",
              "      <td>{'Monday': '11:0-0:0', 'Tuesday': '11:0-0:0', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>39lLJK_rrYY2NYomSsQdUA</td>\n",
              "      <td>Marco's Pizza</td>\n",
              "      <td>24335 Chagrin Blvd</td>\n",
              "      <td>Beachwood</td>\n",
              "      <td>OH</td>\n",
              "      <td>44122</td>\n",
              "      <td>41.465789</td>\n",
              "      <td>-81.506349</td>\n",
              "      <td>2.5</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>{'RestaurantsDelivery': 'True', 'GoodForKids':...</td>\n",
              "      <td>Restaurants, Pizza</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0y6alZmSLnPzmG5_kP5Quw</td>\n",
              "      <td>J J's Pizza</td>\n",
              "      <td>20542 Lorain Rd</td>\n",
              "      <td>Fairview Park</td>\n",
              "      <td>OH</td>\n",
              "      <td>44126</td>\n",
              "      <td>41.448341</td>\n",
              "      <td>-81.847644</td>\n",
              "      <td>4.5</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>{'NoiseLevel': 'u'quiet'', 'WiFi': ''no'', 'Bu...</td>\n",
              "      <td>Pizza, Italian, Restaurants</td>\n",
              "      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                business_id  ...                                              hours\n",
              "63   ZkzutF0P_u0C0yTulwaHkA  ...  {'Monday': '0:0-0:0', 'Tuesday': '11:0-21:0', ...\n",
              "86   OWkS1FXNJbozn-qPg3LWxg  ...                             {'Friday': '18:0-0:0'}\n",
              "105  -C0AlwLuXpcP609madJZQQ  ...  {'Monday': '11:0-0:0', 'Tuesday': '11:0-0:0', ...\n",
              "120  39lLJK_rrYY2NYomSsQdUA  ...                                               None\n",
              "126  0y6alZmSLnPzmG5_kP5Quw  ...  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBuz-3W38nq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6177aceb-8549-46fd-c5b2-8405836f3ba4"
      },
      "source": [
        "pizza_business_ids = set(pizza_biz_df.business_id)\n",
        "pizza_reviews = []\n",
        "with open('yelp_academic_dataset_review.json', 'r') as f:\n",
        "    for line in f:\n",
        "        line_json = json.loads(line)\n",
        "        if line_json['business_id'] in pizza_business_ids:\n",
        "            pizza_reviews.append(line_json)\n",
        "print(f\"And a total of {len(pizza_reviews)} reviews\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And a total of 479792 reviews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsioQyhf0hCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_ids_with_photos = list(os.listdir(\"review_photos\"))\n",
        "pizza_reviews_by_id = {pizza_review[\"review_id\"]: pizza_review for pizza_review in pizza_reviews}\n",
        "output_path = \"review_photos_by_stars\"\n",
        "os.makedirs(os.path.join(output_path, \"0\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_path, \"1\"), exist_ok=True)\n",
        "for review_id_with_photos in review_ids_with_photos:\n",
        "    review_path = os.path.join(\"review_photos\", review_id_with_photos)\n",
        "    if pizza_reviews_by_id[review_id_with_photos][\"stars\"] in [1.0, 5.0]:\n",
        "        for filename in os.listdir(review_path):\n",
        "            img_path = os.path.join(review_path, filename)\n",
        "            copyfile(img_path, os.path.join(output_path, str((int(pizza_reviews_by_id[review_id_with_photos][\"stars\"]) - 1) // 4), f\"{review_id_with_photos}_{filename}\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msdnhjq3_pcu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb4a9b4e-4d2e-4ba5-b7b6-316a0378ba5d"
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4320, 10246)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHSto-oN8e49",
        "colab_type": "text"
      },
      "source": [
        "We are left wtih 4,320 photos from 1-star reviews and 10,246 photos of 5 star reviews. To make training and evaluation easier, we will drop the excess of 5 star review photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ2lJ56V_5jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(max(0, len(os.listdir(os.path.join(output_path, \"1\"))) - len(os.listdir(os.path.join(output_path, \"0\"))))):\n",
        "    os.unlink(os.path.join(output_path, \"1\", np.random.choice(os.listdir(os.path.join(output_path, \"1\")))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33gY_hqUAfvC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3929ead-30a7-4b6e-e719-cb68644385c3"
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4320, 4320)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ET1G_Dy09R",
        "colab_type": "text"
      },
      "source": [
        "We will split the data for training, validation and test sets with 60%:20%:20% respectively. Unlike the proposal, the dataset no longer fits the RAM so we will prepare directories for each subset and feed the model from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYPpGFZJ_EG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c121e202-859d-4be6-e603-b87b94316dc9"
      },
      "source": [
        "def split_train_validation_test(input_path, output_path, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    subsets = [\"train\", \"val\", \"test\"]\n",
        "    train_samples = 0\n",
        "    validation_samples = 0\n",
        "    test_samples = 0\n",
        "    for subset in subsets:\n",
        "        os.makedirs(os.path.join(output_path, subset, \"0\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_path, subset, \"1\"), exist_ok=True)\n",
        "    for class_name in os.listdir(input_path):\n",
        "        class_samples = os.listdir(os.path.join(input_path, class_name))\n",
        "        np.random.shuffle(class_samples)\n",
        "        for class_sample in class_samples[:int(len(class_samples) * 0.6)]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"train\", class_name, class_sample))\n",
        "            train_samples += 1\n",
        "        for class_sample in class_samples[int(len(class_samples) * 0.6):int(len(class_samples) * 0.8)]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"val\", class_name, class_sample))\n",
        "            validation_samples += 1\n",
        "        for class_sample in class_samples[int(len(class_samples) * 0.8):]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"test\", class_name, class_sample))\n",
        "            test_samples += 1\n",
        "    print(f\"Splitted data into [training={train_samples}, validation={validation_samples}, test={test_samples}]\")\n",
        "        \n",
        "split_train_validation_test(\"review_photos_by_stars\", \"review_photos_split\")        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitted data to [training=5184, validation=1728, test=1728]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOHOREZHSQ7_",
        "colab_type": "text"
      },
      "source": [
        "We will now try to compare different available pretrained models on our validation set. We will assume the best pretrained model will perform well with other architectures as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgSDJivT83hr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "ed854a7a-6289-4179-a1d0-066246fa5c21"
      },
      "source": [
        "%%capture\n",
        "DEFAULT_INPUT_SIZE = 500\n",
        "pretrained_networks =  [\n",
        "  ('DenseNet121', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('DenseNet169', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('DenseNet201', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('InceptionResNetV2', tensorflow.keras.applications.inception_resnet_v2.preprocess_input),\n",
        "  ('InceptionV3', tensorflow.keras.applications.inception_v3.preprocess_input),\n",
        "  ('MobileNet', tensorflow.keras.applications.mobilenet.preprocess_input),\n",
        "  ('MobileNetV2', tensorflow.keras.applications.mobilenet_v2.preprocess_input),\n",
        "  ('NASNetLarge', tensorflow.keras.applications.nasnet.preprocess_input),\n",
        "  ('NASNetMobile', tensorflow.keras.applications.nasnet.preprocess_input),\n",
        "  ('ResNet101', tensorflow.keras.applications.resnet.preprocess_input),\n",
        "  ('ResNet101V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('ResNet152', tensorflow.keras.applications.resnet.preprocess_input),\n",
        "  ('ResNet152V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('ResNet50', tensorflow.keras.applications.resnet50.preprocess_input),\n",
        "  ('ResNet50V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('VGG16', tensorflow.keras.applications.vgg16.preprocess_input),\n",
        "  ('VGG19', tensorflow.keras.applications.vgg19.preprocess_input),\n",
        "  ('Xception', tensorflow.keras.applications.xception.preprocess_input)\n",
        "]\n",
        "patience = 5\n",
        "BATCH_SIZE = 64\n",
        "pretrained_network_results = {}\n",
        "for (pretrained_network_name, pretrained_network_preprocessing) in pretrained_networks:\n",
        "    fix_random()\n",
        "    K.clear_session()\n",
        "    try:\n",
        "        pretrained_network = getattr(tensorflow.keras.applications, pretrained_network_name)\n",
        "        try:\n",
        "            input_shape = (DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE)\n",
        "            base_model = pretrained_network(include_top=False, pooling='avg', input_shape=(input_shape[0], input_shape[1], 3))\n",
        "        except Exception:\n",
        "            # Maybe the model has a specific input shape\n",
        "            base_model = pretrained_network(include_top=False, pooling='avg')\n",
        "            input_shape = (base_model.input_shape[1], base_model.input_shape[2])\n",
        "        train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "            directory=\"review_photos_split/train\", \n",
        "            class_mode=\"binary\", \n",
        "            target_size=input_shape, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=False\n",
        "        )\n",
        "        X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "        y_train = train_generator.classes\n",
        "        validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "            directory=\"review_photos_split/val\", \n",
        "            class_mode=\"binary\", \n",
        "            target_size=input_shape, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=False\n",
        "        )\n",
        "        X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "        y_validation = validation_generator.classes\n",
        "        K.clear_session()\n",
        "        input_layer = Input(shape=(base_model.output_shape[1], ))\n",
        "        x = Dense(8, activation='relu')(input_layer)\n",
        "        predictions = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "        h = model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            validation_data = (X_validation, y_validation), \n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1000, \n",
        "            callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "            verbose=0\n",
        "        )\n",
        "        pretrained_network_results[pretrained_network_name] = h.history['val_accuracy'][-patience - 1]\n",
        "    except Exception:\n",
        "      print(f\"Warning - failed to execute {pretrained_network_name}: {format_exc()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d2cc74e14615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_INPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_INPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Maybe the model has a specific input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/densenet.py\u001b[0m in \u001b[0;36mDenseNet121\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    324\u001b[0m   \u001b[0;34m\"\"\"Instantiates the Densenet121 architecture.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m   return DenseNet([6, 12, 24, 16], include_top, weights, input_tensor,\n\u001b[0;32m--> 326\u001b[0;31m                   input_shape, pooling, classes)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/densenet.py\u001b[0m in \u001b[0;36mDenseNet\u001b[0;34m(blocks, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1/conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m   x = layers.BatchNormalization(\n\u001b[1;32m    223\u001b[0m       \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.001e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1/bn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         shape=None):\n\u001b[1;32m    197\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2599\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1432\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1565\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1567\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1569\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    120\u001b[0m       \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     return op(\n\u001b[0;32m-> 1068\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;31m# In case of [0,1) floating results, minval and maxval is unused. We do an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# `is` comparison here since this is cheaper than isinstance or  __eq__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;31m# not convertible to Tensors because of mixed content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    319\u001b[0m                                          as_ref=False):\n\u001b[1;32m    320\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m           pywrap_tfe.TFE_ContextOptionsSetLazyRemoteInputsCopy(\n\u001b[1;32m    514\u001b[0m               opts, self._lazy_remote_inputs_copy)\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwRusU9Rmy2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "67637589-12ba-4e23-8080-538b9a97b8f3"
      },
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "sorted_pretrained_network_results = sorted(pretrained_network_results.items(), key=lambda result_item: result_item[1])\n",
        "barplot = sns.barplot(x=[network_name for (network_name, _) in sorted_pretrained_network_results], y=[network_result for (_, network_result) in sorted_pretrained_network_results])\n",
        "_, pretrained_network_result_values = zip(*sorted_pretrained_network_results)\n",
        "barplot.set(ylim=(min(pretrained_network_result_values) - 0.01, max(pretrained_network_result_values) + 0.01))\n",
        "plt.ylabel(\"Validation Accuracy\", fontsize=14)\n",
        "plt.xlabel(\"Pretrained Network\", fontsize=14)\n",
        "for i, p in enumerate(barplot.patches):\n",
        "    barplot.text(i, p.get_height() + 0.0005, '%.2f%%' % (sorted_pretrained_network_results[i][1] * 100, ), color='black', ha=\"center\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTWgb24iUXOw",
        "colab_type": "text"
      },
      "source": [
        "As you can see DenseNet201 performed the best on our validation set with 73.26% accuracy.\n",
        "From now on we will use the extracted features from DenseNet201 as the base for different algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jfTtS5fzGC",
        "colab_type": "text"
      },
      "source": [
        "### Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRPlJLfEVcjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "30d628ff-7ced-46a9-8c14-8553d59e7e40"
      },
      "source": [
        "K.clear_session()\n",
        "# Create the base pre-trained model\n",
        "base_model = tensorflow.keras.applications.DenseNet201(include_top=False, input_shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3), pooling='avg')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "test_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/test\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "# Precalculate extracted features for the different sets once to save time\n",
        "X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "y_train = train_generator.classes\n",
        "X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "y_validation = validation_generator.classes\n",
        "X_test = base_model.predict(test_generator, batch_size=BATCH_SIZE)\n",
        "y_test = test_generator.classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5184 images belonging to 2 classes.\n",
            "Found 1728 images belonging to 2 classes.\n",
            "Found 1728 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avgwO25enqbg",
        "colab_type": "text"
      },
      "source": [
        "We will try to use a Dropout layer to avoid overfitting on the training set. \n",
        "\n",
        "We also decided to drop the data augmentation, since it made the training very slow and we didn't see a significant improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neKKxATLfGuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1109e32a-073f-4295-9338-c2196e68343e"
      },
      "source": [
        "fix_random()\n",
        "patience = 15\n",
        "BATCH_SIZE = 128\n",
        "input_layer = Input(shape=(base_model.output_shape[1], ))\n",
        "x = Dropout(0.5)(input_layer)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "h = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_validation, y_validation), \n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best epoch: loss: 0.49036961793899536, accuracy: 0.7602237462997437, val_loss: 0.5243268609046936, val_accuracy: 0.7494212985038757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5c_SHlznKnr",
        "colab_type": "text"
      },
      "source": [
        "We can see an improvement over the original result (74.94% compared to 73.26%).\n",
        "\n",
        "Let's try to fine-tune the model to further improve the results. We will recreate the model with the trained weights, unfreeze the last few layers in the base model, and use a low learning rate to avoid destroying the pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd4hIC6dmWoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "input_layer = Input(shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3))\n",
        "x = Dense(8, activation='relu')(base_model(input_layer, training=False))\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "fine_tune_model = Model(inputs=input_layer, outputs=predictions)\n",
        "fine_tune_model.layers[-1].set_weights(model.layers[-1].get_weights())\n",
        "fine_tune_model.layers[-2].set_weights(model.layers[-2].get_weights())\n",
        "for layer in base_model.layers[:-9]:\n",
        "    layer.trainable = False\n",
        "fine_tune_model.compile(optimizer=SGD(learning_rate=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "patience = 3\n",
        "train_generator = ImageDataGenerator(\n",
        "    preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input,\n",
        ").flow_from_directory(\n",
        "    directory=\"review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=42\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=42\n",
        ")\n",
        "h = fine_tune_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucyOayELU9k4",
        "colab_type": "text"
      },
      "source": [
        "### Support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBHfROTU8dY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "6ee408ca-4e18-4e7f-fd83-b1beb8f25e6a"
      },
      "source": [
        "fix_random()\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_validation_predicted = svm.predict(X_validation)\n",
        "print(f\"Validation accuracy for SVM over pretrained model is: {np.sum(y_validation_predicted == y_validation) / y_validation.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy for SVM over pretrained model is: 0.7453703703703703\n",
            "Validation set confusion matrix:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted: 1 star</th>\n",
              "      <th>Predicted: 5 stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True: 1 star</th>\n",
              "      <td>629</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True: 5 stars</th>\n",
              "      <td>205</td>\n",
              "      <td>659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted: 1 star  Predicted: 5 stars\n",
              "True: 1 star                 629                 235\n",
              "True: 5 stars                205                 659"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvAdgEQD1M0Z",
        "colab_type": "text"
      },
      "source": [
        "SVM performed better than the neural network on the validation set. Let's see if it has the same accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASXozUgYp1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e2b6aeeb-e200-4ff2-a873-5bff620430b3"
      },
      "source": [
        "y_test_predicted = svm.predict(X_test)\n",
        "print(f\"Test accuracy for the SVM over pretrained model is: {np.sum(y_test_predicted == y_test) / y_test.shape[0]}\")\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_predicted), index=['True: 1 star', 'True: 5 stars'], columns=['Predicted: 1 star', 'Predicted: 5 stars'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for the SVM over pretrained model is: 0.7806712962962963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted: 1 star</th>\n",
              "      <th>Predicted: 5 stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True: 1 star</th>\n",
              "      <td>655</td>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True: 5 stars</th>\n",
              "      <td>170</td>\n",
              "      <td>694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted: 1 star  Predicted: 5 stars\n",
              "True: 1 star                 655                 209\n",
              "True: 5 stars                170                 694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tth6Sy9gsf6j",
        "colab_type": "text"
      },
      "source": [
        "It seems like the test set accuracy is similar to the validation accuracy which probably indicates that the model has generalized pretty well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYRRLyWXAQai",
        "colab_type": "text"
      },
      "source": [
        "### NLP Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ruz0EdAC4a",
        "colab_type": "text"
      },
      "source": [
        "In this section we aim to identify distinct topics discussed in the corpus of pizza review texts. Once topics are identified, reviews that do not contain food-related topics can be filtered out. This has the potential to improve the image classification in the next stage by weeding out irrelevent samples - where reviewers didn't base their scores on the pizza. \n",
        "\n",
        "This will be achieved by building a topic model that categorizes the information present in each review. A topic can be modeled as a set of words that are all related. For instance, we might say that [noise, smell, music, dirt, lighting] reflects the topic of restaurant atmosphere.  \n",
        "\n",
        "The first algorithm we will utilize is latent Dirichlet allocation. The premise of LDA (Biel et al., 2003) is that documents with similar topics use similar words. The algorithm aims to discover groups of words the occur frequently occur together in the same document. A topic is modeled as a probability distribution over words. Moreover, a document can be modeled as a probability distribution over different topics. \n",
        "\n",
        "Thus, the algorithm works as follows:\n",
        "\n",
        "1.   Remove unimportant words and set how many topics to find.\n",
        "2.   Randomly assign each word in each document to a random topic\n",
        "3.   For each document,\n",
        ">a. choose a topic, assuming all others are allocated correctly\n",
        "\n",
        ">>i. calculate the topic distribution within the document:  p(topic | document)\n",
        "\n",
        ">>ii. calculate the word distribution within the topic: p(word | topic)\n",
        "\n",
        ">> iii. multiply i and ii together and assign words to new topics based on the result\n",
        "\n",
        "4. terminate when there are no new assignments\n",
        "\n",
        "The LDA model is finetuned by several parameters:\n",
        "Alpha reflects how many topics are in a given document (higher values lead to more topics per document in the model)\n",
        "Beta reflects how many words are in a given topic (higher values lead to more words per topic in the model). In this implementation, the model is set to learn these values automatically.\n",
        "\n",
        "We compare the pure LDA model with a hybrid version that utilizes sentence embeddings of the review texts crafted by BERT (Devlin et al., 2018). Inspiration for this hybrid model comes from https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032.\n",
        "\n",
        "BERT is a pretrained language model from researchers at Google that utilizes a multiheaded transformer ANN architecture to craft word vectors that learns to capture the meaning of words from the context in which they are found. BERT and its successors have acheived impressive performance on language understatnding tasks like question answering and others. \n",
        "\n",
        "The hybrid model attempted here fuses the topic probability vector from LDA with the review text embedding from BERT to create a hybrid sample. Since the BERT vectors are much larger that the LDA vectors, scaling is performed on the LDA vectors to even out their relative importance. \n",
        "\n",
        "Clustering is then performed to distinguish different topics in the corpus (the default algorithm used here is KMeans). The most frequent words in the cluster become the topic. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uxtOU-NAdOl",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is necessary to create a universal vocabulary for the corpus. Each text is first processed at the string level and then at the word level. \n",
        "\n",
        "Examples:\n",
        "1. fix typos and missing spaces\n",
        "2. remove punctuation, capitalization, and numbers\n",
        "3. remove unimportant words (stopwords)\n",
        "4. stem words so that a fair comparison can be made (for example, salted and salty both become salt)\n",
        "\n",
        "For the sake of brevity, the preproccessing functions are contained in a seperate script located at:\n",
        "https://github.com/shaifuss/data_science_seminar/blob/master/preproccessing.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K82IjiGhCMo2",
        "colab_type": "text"
      },
      "source": [
        "In an attempt to improve the clustering for the hybrid model, dimensionality reduction is performed. An autoencoder is trained on the LDA+BERT vectors. Once training is complete, the middle hidden layer of length 32 is taken as a representation of the original data. This achieves compression of at least 25x. \n",
        "\n",
        "Clustering is performed on the compressed vectors. After clustering, topics are constructed from the top 5 most frequent words in each cluster.\n",
        "\n",
        "For the hybrid model, BERT outputs sentence-level encodings of length 768 (the first output vector corresponding to the CLS token passed in at the start of each text). An LDA vector contains a probability value in range [0.0, 1.0] for each possible topic. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQzgdG14COCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fec6b7d-8b9c-4bdd-be9b-dbcbf3d088ff"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=200,\n",
        "                                        batch_size=128,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBiHU0lC8vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, method, k=4):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 250  # parameter for reletive importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + str(round(time.time(),0))\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        Get vector representations from selected methods\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'LDA':\n",
        "            print('Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20, alpha='auto', )\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            return vec\n",
        "\n",
        "        #         elif method == 'LDA_BERT':\n",
        "        else: \n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('Fitting Autoencoder ...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        Fit the topic model for selected method given the preprocessed data\n",
        "        :docs: list of documents, each doc is preprocessed as tokens\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('Fitting LDA ...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary\n",
        "                                                                , alpha='auto', eta='auto', minimum_probability=0.3)\n",
        "        else:\n",
        "            print('Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "        \"\"\"\n",
        "        Predict topics for new_documents\n",
        "        \"\"\"\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "\n",
        "        print(\"Predicting...\")\n",
        "        print(\"Out of sample is ()\".format(out_of_sample))\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":   # take the most prevalent topic\n",
        "            #lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     #key=lambda x: x[1], reverse=True)[0][0], corpus)))\n",
        "            lbs = []\n",
        "            for text in corpus:\n",
        "              lbs.append(self.ldamodel.get_document_topics(text))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs\n",
        "    def persist(self, workdir):\n",
        "      with open(os.path.join(workdir, \"test_\" + self.id), 'wb') as f:\n",
        "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjdLrj_5DGvw",
        "colab_type": "text"
      },
      "source": [
        "Coherence is used for topic model evaluation (Roder et al., 2015). As topic models produced algorithmically are sometimes not easily interpreted by humans, it is necessary to have a way to objectively measure how well the words in a topic go together. \n",
        "\n",
        "Very generally, coherence can be evaluated for a topic by first computing a Cartesian product on itself to construct pairs (excluding twin pairs). The co-occurence of each pair is checked in an external reference - a very large corpus of texts said to represent common language usage. The higher the co-occurence scores, the higher the topic coherence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDekpuykvS47",
        "colab_type": "text"
      },
      "source": [
        "We hypothesize that the following four topics are relevant to pizza reviews.\n",
        "\n",
        "1. Food\n",
        "2. Service\n",
        "3. Atmosphere\n",
        "4. Value\n",
        "\n",
        "Adding +/- 1, we train models with 3, 4, and 5 topics for each method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2urPDtaoRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(method, ntopic, sentences, token_lists, idx_in):\n",
        "  \n",
        "  print(\"Starting training...\")\n",
        "  start = time.time()\n",
        "  tm = Topic_Model(method, k = ntopic)\n",
        "  tm.fit(sentences, token_lists)\n",
        "  end = time.time()\n",
        "  print(\"Training on {} reviews took {} minutes\".format(len(sentences), str((end - start)/60)))\n",
        "  \n",
        "  return tm\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQQy8uEoa3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "da23aeba3c1048c99a1b9e6ed2d270d8",
            "03dc14370b3343749733fe9faa727cd5",
            "bdaa02fb0be34833b4881051f954632e",
            "5a912acb496847e9b395509ebbce9ace",
            "4d30078b9dea404a929f028e04dd8d91",
            "efa294928125490f97901b40b033bac2",
            "6da70117fa57481db5785f89c0a29734",
            "71737015f401430d8cdf69f9fe120a49",
            "3f3923a126064938b04f7aaf3a668e3c",
            "2dc138e3b7a94e20b92bd731562a5685",
            "b9c68ab143fe4cd483b5bf022e27ace9",
            "f40f15854ca34799bca81593e18025f1",
            "be930335918e4db7b3158aebcf599f23",
            "c02f8da18a8943559cff512243ccbcdf",
            "3adcf86845a14d4fa6f0915d7c25ad13",
            "87c77ad7ea914d6aaf9dc94e2a4af940",
            "d7f6cdbd4c5347878cdc0b499676f0d2",
            "7c14264015d944938eb93eefa236e366",
            "8650b1f3842e49f0809ae4448b3d145b",
            "af570ed098a742c999a4790a144138ff",
            "b0ed14644a7d4aa9b671e409ea67058e",
            "47ca6483e8874c729a50cf37ba4f3f14",
            "e9fa70ad085d420f87241ae6c350a4d9",
            "f1bfae2491004246b1c07e839f67ae0a"
          ]
        },
        "outputId": "789b100b-907c-4cdf-d771-12e8354edb88"
      },
      "source": [
        "import preprocessing\n",
        "import postprocess\n",
        "\n",
        "pic_path = 'review_photos'\n",
        "pix_review_ids = [f for f in os.listdir(pic_path) if os.path.isdir(os.path.join(pic_path, f))]\n",
        "\n",
        "text_df = pd.DataFrame.from_records(pizza_reviews)\n",
        "\n",
        "pic_review_df = text_df[text_df['review_id'].isin(pix_review_ids)]\n",
        "pic_review_df = pic_review_df.reset_index(drop=True)\n",
        "pic_review_df = pic_review_df.fillna('')\n",
        "reviews = pic_review_df.text\n",
        "\n",
        "sentences, token_lists, idx_in = preprocessing.preprocess(reviews, len(reviews) + 1)\n",
        "\n",
        "model_dict = dict()\n",
        "for method in [\"LDA\", \"LDA_BERT\"]:\n",
        "  for num_topics in range(3, 6):\n",
        "    tm = train(method, num_topics, sentences, token_lists, idx_in)\n",
        "    model_dict[method + \"_\" + str(num_topics)] = dict()\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"model\"] = tm\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"coherence\"] = postprocess.get_coherence(tm, token_lists, 'c_v')\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"silhouette\"] = postprocess.get_silhouette(tm)\n",
        "    if method == \"LDA\":\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = postprocess.get_topic_words_lda(tm)\n",
        "    else:\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = postprocess.get_topic_words_hybrid(token_lists, tm.cluster_model.labels_, k=None)\n",
        "      postprocess.visualize(tm)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Stage 1: Preprocess raw review texts\n",
            "Preprocessing 10 reviews took 0.14697365760803222 minutes\n",
            "Starting training...\n",
            "Fitting LDA ...\n",
            "Training on 10 reviews took 0.0007371902465820312 minutes\n",
            "[['food', 'slice', 'pizza', 'servic', 'dog'], ['pizza', 'food', 'wing', 'visit', 'order'], ['pizza', 'pie', 'casino', 'food', 'basil']]\n",
            "Starting training...\n",
            "Fitting LDA ...\n",
            "Training on 10 reviews took 0.0006344755490620931 minutes\n",
            "[['pizza', 'food', 'slice', 'prosciutto', 'way'], ['food', 'pizza', 'dinner', 'notch', 'pie'], ['food', 'servic', 'pizza', 'wing', 'ask'], ['pizza', 'slice', 'dog', 'servic', 'visit']]\n",
            "Starting training...\n",
            "Fitting LDA ...\n",
            "Training on 10 reviews took 0.0006121198336283366 minutes\n",
            "[['food', 'pizza', 'pie', 'slice', 'notch'], ['pizza', 'order', 'prosciutto', 'wing', 'thick'], ['dog', 'servic', 'vega', 'food', 'place'], ['pizza', 'slice', 'visit', 'servic', 'wing'], ['food', 'pizza', 'pie', 'everyth', 'casino']]\n",
            "Starting training...\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da23aeba3c1048c99a1b9e6ed2d270d8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=2.0, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting Autoencoder ...\n",
            "Training on 10 reviews took 0.08384005228678386 minutes\n",
            "Topics are:\n",
            "[['pizza', 'slice', 'order', 'wing', 'lot'], ['food', 'pizza', 'servic', 'visit', 'anna'], ['food', 'pizza', 'pie', 'casino', 'basil']]\n",
            "Calculating UMAP projection ...\n",
            "Starting training...\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f3923a126064938b04f7aaf3a668e3c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=2.0, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting Autoencoder ...\n",
            "Training on 10 reviews took 0.08853114048639933 minutes\n",
            "Topics are:\n",
            "[['wing', 'dog', 'manag', 'ask', 'thick'], ['pizza', 'food', 'prosciutto', 'centurion', 'bar'], ['slice', 'pizza', 'food', 'servic', 'thing'], ['pizza', 'pie', 'casino', 'basil', 'food']]\n",
            "Calculating UMAP projection ...\n",
            "Starting training...\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7f6cdbd4c5347878cdc0b499676f0d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=2.0, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting Autoencoder ...\n",
            "Training on 10 reviews took 0.07895282109578451 minutes\n",
            "Topics are:\n",
            "[['pizza', 'prosciutto', 'dog', 'order', 'vega'], ['place', 'visit', 'food', 'servic', 'dish'], ['pizza', 'slice', 'food', 'servic', 'visit'], ['pizza', 'pie', 'casino', 'basil', 'food'], ['food', 'centurion', 'bar', 'dinner', 'room']]\n",
            "Calculating UMAP projection ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph1S4BbBbGEJ",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "1. Coherence was consistently an order of magnitude higher for the pure LDA model. \n",
        "2. Silhoutte scores improved as the LDA vectors were weighted more heavily in the hybrid model\n",
        "3. The pure LDA model produced higher quality topics as judged by two human evaluators (us). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR5CIIYosI7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "70159d18-c335-4d01-be01-5bf931685500"
      },
      "source": [
        "stemmed_pizza_vocab = preprocessing.stem_words([\"crust\", \"slice\", \"sauce\", \"topping\", \"cheese\", \"tomato\", \"food\", \"pie\"])\n",
        "model_dict = postprocess.get_scores(model_dict, stemmed_pizza_vocab)\n",
        "best_method = max(model_dict, key=lambda v: model_dict[v]['coherence'])\n",
        "best_topic = max(range(len(model_dict[best_method][\"scores\"])), key=model_dict[best_method][\"scores\"].__getitem__)\n",
        "print(\"best method is {}\".format(best_method))\n",
        "print(\"best_topic is at index {}\".format(best_topic))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method: LDA_3, topic scores: [15, 14, 16]\n",
            "method: LDA_3, coherence: 0.4056398255629527\n",
            "method: LDA_4, topic scores: [17, 14, 11, 14]\n",
            "method: LDA_4, coherence: 0.34984224464717656\n",
            "method: LDA_5, topic scores: [18, 10, 2, 14, 16]\n",
            "method: LDA_5, coherence: 0.4425747141211235\n",
            "method: LDA_BERT_3, topic scores: [14, 13, 16]\n",
            "method: LDA_BERT_3, coherence: 0.5144966058574432\n",
            "method: LDA_BERT_3, silhouette: 0.7417293787002563\n",
            "method: LDA_BERT_4, topic scores: [0, 14, 16, 15]\n",
            "method: LDA_BERT_4, coherence: 0.5141876811282327\n",
            "method: LDA_BERT_4, silhouette: 0.8018933534622192\n",
            "method: LDA_BERT_5, topic scores: [10, 3, 17, 15, 5]\n",
            "method: LDA_BERT_5, coherence: 0.6015261847188073\n",
            "method: LDA_BERT_5, silhouette: 0.6521129608154297\n",
            "best method is LDA_BERT_5\n",
            "best_topic is at index 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P02iiJwSQWYI",
        "colab_type": "text"
      },
      "source": [
        "For prediction with the pure LDA model, the topics are retrieved for a given review. If the best pizza topic is not the most prominent topic in that review, our hypothesis is that discarding it will lead to improved performance for the image classifier.\n",
        "\n",
        "For prediction with the LDA+BERT version, a review is place into a cluster with the pretrained model. Similar to the pure model, we should keep the reviews that fall into the best pizza topic cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZNfuv3QVgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "print(\"There are {} reviews before filtering\".format(len(sentences)))\n",
        "topic_lists = model_dict[best_method][\"model\"].predict(sentences, token_lists, True)\n",
        "indicies = []\n",
        "print(topic_lists)\n",
        "for i, topic_list in enumerate(topic_lists):\n",
        "  topics = [x[0] for x in topic_list]\n",
        "  if best_topic in topics:\n",
        "    indicies.append(i)\n",
        "print(\"There are {} reviews afer filtering\".format(len(indicies)))\n",
        "\n",
        "food_based_reviews = pic_review_df[pic_review_df.index.isin(indicies)]\n",
        "print(\"Example reviews that passed the filter:\\n\")\n",
        "print(food_based_reviews.loc[random.choice(indicies), \"text\"])\n",
        "print(\"---------------------------------------------------------------\")\n",
        "print(food_based_reviews.loc[random.choice(indicies), \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSlnMPhyJyc2",
        "colab_type": "text"
      },
      "source": [
        "TODO: food_based_reviews has the review ids that passed filtering. take those and rerun the image classifier"
      ]
    }
  ]
}