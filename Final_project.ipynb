{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Vn1s8eQR5J",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Online Review Ratings of Pizzarias Using Review Photos \n",
        "\n",
        "Ziv Branstein 301782215\n",
        "\n",
        "Avishai Fuss 332658608"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px_4ubGwSHmx",
        "colab_type": "text"
      },
      "source": [
        "## Scraping\n",
        "As discussed in the project proposal Yelp's dataset does not include photos of reviews, so we had to scrape the photos for our project.\n",
        "For the project proposal we used a basic web scraper, after analyzing the web traffic we used the same requests as their website to download the photos of reviews found in the dataset. This worked well, however, after a few hundred reviews scraped we encountered a consistent \"503 service unavailable\" response, checking their website revealed that we were blocked:\n",
        "![Yelp's ip block](https://github.com/shaifuss/data_science_seminar/blob/master/yelp-ip-address-ban.jpg?raw=1)\n",
        "After disconnecting from the internet and acquiring a new IP address the website was working again, meaning the blocking was done based on the IP.\n",
        "This was a real problem, as there are 479792 reviews in total, and using only a few hundred photos for the project will not produce meaningful results.\n",
        "After getting blocked again we tested the mobile application and it was still operating, giving hope for a new direction for scraping.\n",
        "Analyzing the traffic for the mobile application was a bit more cumbersome - Yelp's app uses SSL pinning(a technique which prevents man-in-the-middle attack for reading the traffic) that needed to be disabled. Also, the HTTP requests performed by the app were signed using logic run inside the app which needed to be reverse engineered.\n",
        "The signature algorithm of the android app uses HMAC-SHA1 on the query string with a key embedded in the binary.\n",
        "\n",
        "The scraping process is performed as follows: \n",
        "```\n",
        "review_scaper.py -> image_scraper.py -> pizza_classifier_xception.py\n",
        "review_scraper.py: For each review it scrapes the urls of the images of the review.\n",
        "image_scraper.py: Given the scraped urls it downloads the photos\n",
        "pizza_classifier_xception.py: Deletes photos which are not classified as pizza in the top 5 results for the Xception classifier\n",
        "```\n",
        "After reverse engineering the signature mechanism we retested the scraping and managed to scrape all 1 star reviews from the dataset and a similar amount of 5 star reviews.\n",
        "After filtering for images of pizzas only (using the pretrained network Xception) we are left with 14,566 photos of pizzas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdXgQf9EUA_n",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA4QlKGA8YTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "from collections import defaultdict\n",
        "from IPython.display import Image\n",
        "from scipy.stats import binom\n",
        "from shutil import copyfile, move\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from traceback import format_exc\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import random\n",
        "import seaborn as sns \n",
        "import tensorflow\n",
        "import tensorflow.keras.applications\n",
        "\n",
        "!pip install pyspellchecker -q\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "# Fix random for consistent results\n",
        "def fix_random():\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    tensorflow.random.set_seed(42)\n",
        "fix_random()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4I-ydg8e9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fa7d063-e745-42c2-cfd0-09742e977b44"
      },
      "source": [
        "if not os.path.exists('review_photos'):\n",
        "    if os.path.exists('data_science_seminar'):\n",
        "        %cd data_science_seminar\n",
        "    else:\n",
        "        !git clone https://github.com/shaifuss/data_science_seminar.git\n",
        "        %cd data_science_seminar\n",
        "if (not os.path.exists('yelp_academic_dataset_business.json')) or (not os.path.exists('yelp_academic_dataset_review.json')):\n",
        "    kaggle_path = os.path.expanduser('~/.kaggle')\n",
        "    kaggle_json_path = os.path.join(kaggle_path, 'kaggle.json')\n",
        "    if not os.path.exists(kaggle_json_path):\n",
        "        from getpass import getpass\n",
        "        kaggle_json = getpass('Insert kaggle.json:')\n",
        "        os.makedirs(kaggle_path, exist_ok=True)\n",
        "        with open(kaggle_json_path, 'w') as f:\n",
        "            f.write(kaggle_json)\n",
        "        os.chmod(kaggle_json_path, 0o600)\n",
        "    !kaggle datasets download yelp-dataset/yelp-dataset\n",
        "    !unzip yelp-dataset.zip yelp_academic_dataset_business.json yelp_academic_dataset_review.json\n",
        "    !rm yelp-dataset.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data_science_seminar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfjKh1V8imk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f936366-432a-40d9-e5a5-e20efbca1aa1"
      },
      "source": [
        "business_df = pd.read_json('yelp_academic_dataset_business.json', lines=True)\n",
        "print(\"The dataset contains a total of {} businesses\".format(len(business_df.index)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset contains a total of 209393 businesses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yPZKfSk-QA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "493f9e9f-590e-4a9f-8359-72225e1d654d"
      },
      "source": [
        "business_df = business_df[business_df['categories'].notna()]\n",
        "pizza_biz_df = business_df[business_df['categories'].str.contains(\"Pizza\")]\n",
        "print(\"Of those, {} businesses sell pizza\".format(len(pizza_biz_df.index)))\n",
        "print(\"Here are a few examples\")\n",
        "pizza_biz_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Of those, 7302 businesses sell pizza\n",
            "Here are a few examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>name</th>\n",
              "      <th>address</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>postal_code</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>is_open</th>\n",
              "      <th>attributes</th>\n",
              "      <th>categories</th>\n",
              "      <th>hours</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>ZkzutF0P_u0C0yTulwaHkA</td>\n",
              "      <td>Lelulos Pizzeria</td>\n",
              "      <td>311 Unity Center Rd</td>\n",
              "      <td>Plum</td>\n",
              "      <td>PA</td>\n",
              "      <td>15239</td>\n",
              "      <td>40.489996</td>\n",
              "      <td>-79.779288</td>\n",
              "      <td>4.0</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>{'RestaurantsPriceRange2': '1', 'BusinessAccep...</td>\n",
              "      <td>Restaurants, Pizza</td>\n",
              "      <td>{'Monday': '0:0-0:0', 'Tuesday': '11:0-21:0', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>OWkS1FXNJbozn-qPg3LWxg</td>\n",
              "      <td>Mama Napoli Pizza</td>\n",
              "      <td></td>\n",
              "      <td>Las Vegas</td>\n",
              "      <td>NV</td>\n",
              "      <td>89109</td>\n",
              "      <td>36.128561</td>\n",
              "      <td>-115.171130</td>\n",
              "      <td>4.5</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>{'RestaurantsDelivery': 'False', 'BusinessAcce...</td>\n",
              "      <td>Food, Food Trucks, Restaurants, Pizza</td>\n",
              "      <td>{'Friday': '18:0-0:0'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-C0AlwLuXpcP609madJZQQ</td>\n",
              "      <td>Pizzaville</td>\n",
              "      <td>1030 Kennedy Circle, Unit 10</td>\n",
              "      <td>Milton</td>\n",
              "      <td>ON</td>\n",
              "      <td>L9T 0J9</td>\n",
              "      <td>43.508962</td>\n",
              "      <td>-79.837990</td>\n",
              "      <td>3.5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>None</td>\n",
              "      <td>Restaurants, Pizza</td>\n",
              "      <td>{'Monday': '11:0-0:0', 'Tuesday': '11:0-0:0', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>39lLJK_rrYY2NYomSsQdUA</td>\n",
              "      <td>Marco's Pizza</td>\n",
              "      <td>24335 Chagrin Blvd</td>\n",
              "      <td>Beachwood</td>\n",
              "      <td>OH</td>\n",
              "      <td>44122</td>\n",
              "      <td>41.465789</td>\n",
              "      <td>-81.506349</td>\n",
              "      <td>2.5</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>{'RestaurantsDelivery': 'True', 'GoodForKids':...</td>\n",
              "      <td>Restaurants, Pizza</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0y6alZmSLnPzmG5_kP5Quw</td>\n",
              "      <td>J J's Pizza</td>\n",
              "      <td>20542 Lorain Rd</td>\n",
              "      <td>Fairview Park</td>\n",
              "      <td>OH</td>\n",
              "      <td>44126</td>\n",
              "      <td>41.448341</td>\n",
              "      <td>-81.847644</td>\n",
              "      <td>4.5</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>{'NoiseLevel': 'u'quiet'', 'WiFi': ''no'', 'Bu...</td>\n",
              "      <td>Pizza, Italian, Restaurants</td>\n",
              "      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                business_id  ...                                              hours\n",
              "63   ZkzutF0P_u0C0yTulwaHkA  ...  {'Monday': '0:0-0:0', 'Tuesday': '11:0-21:0', ...\n",
              "86   OWkS1FXNJbozn-qPg3LWxg  ...                             {'Friday': '18:0-0:0'}\n",
              "105  -C0AlwLuXpcP609madJZQQ  ...  {'Monday': '11:0-0:0', 'Tuesday': '11:0-0:0', ...\n",
              "120  39lLJK_rrYY2NYomSsQdUA  ...                                               None\n",
              "126  0y6alZmSLnPzmG5_kP5Quw  ...  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBuz-3W38nq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "691be04b-afba-4c06-d6b8-06f27ac9d5aa"
      },
      "source": [
        "pizza_business_ids = set(pizza_biz_df.business_id)\n",
        "pizza_reviews = []\n",
        "with open('yelp_academic_dataset_review.json', 'r') as f:\n",
        "    for line in f:\n",
        "        line_json = json.loads(line)\n",
        "        if line_json['business_id'] in pizza_business_ids:\n",
        "            pizza_reviews.append(line_json)\n",
        "print(f\"And a total of {len(pizza_reviews)} reviews\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f869374cde01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_academic_dataset_review.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mline_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpizza_business_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpizza_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 162 (char 161)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsioQyhf0hCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "152c11f4-fe15-44f0-9d0c-688bca84c8e6"
      },
      "source": [
        "review_ids_with_photos = list(os.listdir(\"review_photos\"))\n",
        "pizza_reviews_by_id = {pizza_review[\"review_id\"]: pizza_review for pizza_review in pizza_reviews}\n",
        "output_path = \"review_photos_by_stars\"\n",
        "os.makedirs(os.path.join(output_path, \"0\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_path, \"1\"), exist_ok=True)\n",
        "for review_id_with_photos in review_ids_with_photos:\n",
        "    review_path = os.path.join(\"review_photos\", review_id_with_photos)\n",
        "    if pizza_reviews_by_id[review_id_with_photos][\"stars\"] in [1.0, 5.0]:\n",
        "        for filename in os.listdir(review_path):\n",
        "            img_path = os.path.join(review_path, filename)\n",
        "            copyfile(img_path, os.path.join(output_path, str((int(pizza_reviews_by_id[review_id_with_photos][\"stars\"]) - 1) // 4), f\"{review_id_with_photos}_{filename}\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a79cf052aab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview_id_with_photos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_ids_with_photos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mreview_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"review_photos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_id_with_photos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpizza_reviews_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview_id_with_photos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stars\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'vwxlHhxasrS4X3VlPJrPVg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msdnhjq3_pcu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb4a9b4e-4d2e-4ba5-b7b6-316a0378ba5d"
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4320, 10246)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHSto-oN8e49",
        "colab_type": "text"
      },
      "source": [
        "We are left wtih 4,320 photos from 1-star reviews and 10,246 photos of 5 star reviews. To make training and evaluation easier, we will drop the excess of 5 star review photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ2lJ56V_5jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(max(0, len(os.listdir(os.path.join(output_path, \"1\"))) - len(os.listdir(os.path.join(output_path, \"0\"))))):\n",
        "    os.unlink(os.path.join(output_path, \"1\", np.random.choice(os.listdir(os.path.join(output_path, \"1\")))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33gY_hqUAfvC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3929ead-30a7-4b6e-e719-cb68644385c3"
      },
      "source": [
        "len(os.listdir(os.path.join(output_path, \"0\"))), len(os.listdir(os.path.join(output_path, \"1\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4320, 4320)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ET1G_Dy09R",
        "colab_type": "text"
      },
      "source": [
        "We will split the data for training, validation and test sets with 60%:20%:20% respectively. Unlike the proposal, the dataset no longer fits the RAM so we will prepare directories for each subset and feed the model from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYPpGFZJ_EG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c121e202-859d-4be6-e603-b87b94316dc9"
      },
      "source": [
        "def split_train_validation_test(input_path, output_path, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    subsets = [\"train\", \"val\", \"test\"]\n",
        "    train_samples = 0\n",
        "    validation_samples = 0\n",
        "    test_samples = 0\n",
        "    for subset in subsets:\n",
        "        os.makedirs(os.path.join(output_path, subset, \"0\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_path, subset, \"1\"), exist_ok=True)\n",
        "    for class_name in os.listdir(input_path):\n",
        "        class_samples = os.listdir(os.path.join(input_path, class_name))\n",
        "        np.random.shuffle(class_samples)\n",
        "        for class_sample in class_samples[:int(len(class_samples) * 0.6)]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"train\", class_name, class_sample))\n",
        "            train_samples += 1\n",
        "        for class_sample in class_samples[int(len(class_samples) * 0.6):int(len(class_samples) * 0.8)]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"val\", class_name, class_sample))\n",
        "            validation_samples += 1\n",
        "        for class_sample in class_samples[int(len(class_samples) * 0.8):]:\n",
        "            move(os.path.join(input_path, class_name, class_sample), os.path.join(output_path, \"test\", class_name, class_sample))\n",
        "            test_samples += 1\n",
        "    print(f\"Splitted data into [training={train_samples}, validation={validation_samples}, test={test_samples}]\")\n",
        "        \n",
        "split_train_validation_test(\"review_photos_by_stars\", \"review_photos_split\")        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitted data to [training=5184, validation=1728, test=1728]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOHOREZHSQ7_",
        "colab_type": "text"
      },
      "source": [
        "We will now try to compare different available pretrained models on our validation set. We will assume the best pretrained model will perform well with other architectures as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgSDJivT83hr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "ed854a7a-6289-4179-a1d0-066246fa5c21"
      },
      "source": [
        "%%capture\n",
        "DEFAULT_INPUT_SIZE = 500\n",
        "pretrained_networks =  [\n",
        "  ('DenseNet121', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('DenseNet169', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('DenseNet201', tensorflow.keras.applications.densenet.preprocess_input),\n",
        "  ('InceptionResNetV2', tensorflow.keras.applications.inception_resnet_v2.preprocess_input),\n",
        "  ('InceptionV3', tensorflow.keras.applications.inception_v3.preprocess_input),\n",
        "  ('MobileNet', tensorflow.keras.applications.mobilenet.preprocess_input),\n",
        "  ('MobileNetV2', tensorflow.keras.applications.mobilenet_v2.preprocess_input),\n",
        "  ('NASNetLarge', tensorflow.keras.applications.nasnet.preprocess_input),\n",
        "  ('NASNetMobile', tensorflow.keras.applications.nasnet.preprocess_input),\n",
        "  ('ResNet101', tensorflow.keras.applications.resnet.preprocess_input),\n",
        "  ('ResNet101V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('ResNet152', tensorflow.keras.applications.resnet.preprocess_input),\n",
        "  ('ResNet152V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('ResNet50', tensorflow.keras.applications.resnet50.preprocess_input),\n",
        "  ('ResNet50V2', tensorflow.keras.applications.resnet_v2.preprocess_input),\n",
        "  ('VGG16', tensorflow.keras.applications.vgg16.preprocess_input),\n",
        "  ('VGG19', tensorflow.keras.applications.vgg19.preprocess_input),\n",
        "  ('Xception', tensorflow.keras.applications.xception.preprocess_input)\n",
        "]\n",
        "patience = 5\n",
        "BATCH_SIZE = 64\n",
        "pretrained_network_results = {}\n",
        "for (pretrained_network_name, pretrained_network_preprocessing) in pretrained_networks:\n",
        "    fix_random()\n",
        "    K.clear_session()\n",
        "    try:\n",
        "        pretrained_network = getattr(tensorflow.keras.applications, pretrained_network_name)\n",
        "        try:\n",
        "            input_shape = (DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE)\n",
        "            base_model = pretrained_network(include_top=False, pooling='avg', input_shape=(input_shape[0], input_shape[1], 3))\n",
        "        except Exception:\n",
        "            # Maybe the model has a specific input shape\n",
        "            base_model = pretrained_network(include_top=False, pooling='avg')\n",
        "            input_shape = (base_model.input_shape[1], base_model.input_shape[2])\n",
        "        train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "            directory=\"review_photos_split/train\", \n",
        "            class_mode=\"binary\", \n",
        "            target_size=input_shape, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=False\n",
        "        )\n",
        "        X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "        y_train = train_generator.classes\n",
        "        validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "            directory=\"review_photos_split/val\", \n",
        "            class_mode=\"binary\", \n",
        "            target_size=input_shape, \n",
        "            batch_size=BATCH_SIZE, \n",
        "            shuffle=False\n",
        "        )\n",
        "        X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "        y_validation = validation_generator.classes\n",
        "        K.clear_session()\n",
        "        input_layer = Input(shape=(base_model.output_shape[1], ))\n",
        "        x = Dense(8, activation='relu')(input_layer)\n",
        "        predictions = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=predictions)\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "        h = model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            validation_data = (X_validation, y_validation), \n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1000, \n",
        "            callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "            verbose=0\n",
        "        )\n",
        "        pretrained_network_results[pretrained_network_name] = h.history['val_accuracy'][-patience - 1]\n",
        "    except Exception:\n",
        "      print(f\"Warning - failed to execute {pretrained_network_name}: {format_exc()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d2cc74e14615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_INPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_INPUT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Maybe the model has a specific input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/densenet.py\u001b[0m in \u001b[0;36mDenseNet121\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    324\u001b[0m   \u001b[0;34m\"\"\"Instantiates the Densenet121 architecture.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m   return DenseNet([6, 12, 24, 16], include_top, weights, input_tensor,\n\u001b[0;32m--> 326\u001b[0;31m                   input_shape, pooling, classes)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/densenet.py\u001b[0m in \u001b[0;36mDenseNet\u001b[0;34m(blocks, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1/conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m   x = layers.BatchNormalization(\n\u001b[1;32m    223\u001b[0m       \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.001e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1/bn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         shape=None):\n\u001b[1;32m    197\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2599\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1432\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1565\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1567\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1569\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    120\u001b[0m       \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     return op(\n\u001b[0;32m-> 1068\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;31m# In case of [0,1) floating results, minval and maxval is unused. We do an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# `is` comparison here since this is cheaper than isinstance or  __eq__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;31m# not convertible to Tensors because of mixed content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    319\u001b[0m                                          as_ref=False):\n\u001b[1;32m    320\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m           pywrap_tfe.TFE_ContextOptionsSetLazyRemoteInputsCopy(\n\u001b[1;32m    514\u001b[0m               opts, self._lazy_remote_inputs_copy)\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwRusU9Rmy2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "67637589-12ba-4e23-8080-538b9a97b8f3"
      },
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "sorted_pretrained_network_results = sorted(pretrained_network_results.items(), key=lambda result_item: result_item[1])\n",
        "barplot = sns.barplot(x=[network_name for (network_name, _) in sorted_pretrained_network_results], y=[network_result for (_, network_result) in sorted_pretrained_network_results])\n",
        "_, pretrained_network_result_values = zip(*sorted_pretrained_network_results)\n",
        "barplot.set(ylim=(min(pretrained_network_result_values) - 0.01, max(pretrained_network_result_values) + 0.01))\n",
        "plt.ylabel(\"Validation Accuracy\", fontsize=14)\n",
        "plt.xlabel(\"Pretrained Network\", fontsize=14)\n",
        "for i, p in enumerate(barplot.patches):\n",
        "    barplot.text(i, p.get_height() + 0.0005, '%.2f%%' % (sorted_pretrained_network_results[i][1] * 100, ), color='black', ha=\"center\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTWgb24iUXOw",
        "colab_type": "text"
      },
      "source": [
        "As you can see DenseNet201 performed the best on our validation set with 73.26% accuracy.\n",
        "From now on we will use the extracted features from DenseNet201 as the base for different algorithms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jfTtS5fzGC",
        "colab_type": "text"
      },
      "source": [
        "### Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRPlJLfEVcjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "30d628ff-7ced-46a9-8c14-8553d59e7e40"
      },
      "source": [
        "K.clear_session()\n",
        "# Create the base pre-trained model\n",
        "base_model = tensorflow.keras.applications.DenseNet201(include_top=False, input_shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3), pooling='avg')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "test_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/test\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False\n",
        ")\n",
        "# Precalculate extracted features for the different sets once to save time\n",
        "X_train = base_model.predict(train_generator, batch_size=BATCH_SIZE)\n",
        "y_train = train_generator.classes\n",
        "X_validation = base_model.predict(validation_generator, batch_size=BATCH_SIZE)\n",
        "y_validation = validation_generator.classes\n",
        "X_test = base_model.predict(test_generator, batch_size=BATCH_SIZE)\n",
        "y_test = test_generator.classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5184 images belonging to 2 classes.\n",
            "Found 1728 images belonging to 2 classes.\n",
            "Found 1728 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avgwO25enqbg",
        "colab_type": "text"
      },
      "source": [
        "We will try to use a Dropout layer to avoid overfitting on the training set. \n",
        "\n",
        "We also decided to drop the data augmentation, since it made the training very slow and we didn't see a significant improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neKKxATLfGuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1109e32a-073f-4295-9338-c2196e68343e"
      },
      "source": [
        "patience = 15\n",
        "BATCH_SIZE = 64\n",
        "input_layer = Input(shape=(base_model.output_shape[1], ))\n",
        "x = Dropout(0.5)(input_layer)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "h = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data = (X_validation, y_validation), \n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best epoch: loss: 0.49036961793899536, accuracy: 0.7602237462997437, val_loss: 0.5243268609046936, val_accuracy: 0.7494212985038757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5c_SHlznKnr",
        "colab_type": "text"
      },
      "source": [
        "We can see an improvement over the original result (74.94% compared to 73.26%).\n",
        "\n",
        "Let's try to fine-tune the model to further improve the results. We will recreate the model with the trained weights, unfreeze the last few layers in the base model, and use a low learning rate to avoid destroying the pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd4hIC6dmWoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "e201d90d-ffc2-4eaf-c0a0-f12db29b2e71"
      },
      "source": [
        "K.clear_session()\n",
        "input_layer = Input(shape=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE, 3))\n",
        "x = Dense(16, activation='relu')(base_model(input_layer, training=False))\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "fine_tune_model = Model(inputs=input_layer, outputs=predictions)\n",
        "fine_tune_model.layers[-1].set_weights(model.layers[-1].get_weights())\n",
        "fine_tune_model.layers[-2].set_weights(model.layers[-2].get_weights())\n",
        "for layer in base_model.layers[:-9]:\n",
        "    layer.trainable = False\n",
        "fine_tune_model.compile(optimizer=SGD(learning_rate=0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"]) # TODO: RMSprop(learning_rate=0.0001)?\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "patience = 5\n",
        "train_generator = ImageDataGenerator(\n",
        "    preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input,\n",
        ").flow_from_directory(\n",
        "    directory=\"review_photos_split/train\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=42\n",
        ")\n",
        "validation_generator = ImageDataGenerator(preprocessing_function=tensorflow.keras.applications.densenet.preprocess_input).flow_from_directory(\n",
        "    directory=\"review_photos_split/val\", \n",
        "    class_mode=\"binary\", \n",
        "    target_size=(DEFAULT_INPUT_SIZE, DEFAULT_INPUT_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    seed=42\n",
        ")\n",
        "h = fine_tune_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=1000, \n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5184 images belonging to 2 classes.\n",
            "Found 1728 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-b5024c2d2c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best epoch: loss: {h.history['loss'][-patience - 1]}, accuracy: {h.history['accuracy'][-patience - 1]}, val_loss: {h.history['val_loss'][-patience - 1]}, val_accuracy: {h.history['val_accuracy'][-patience - 1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m                 ))\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_function'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_per_replica\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    949\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m    950\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m--> 951\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[0;31m# TODO(b/151224785): Remove deprecated alias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2649\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;31m# such as loss scaling and gradient clipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n\u001b[0;32m--> 541\u001b[0;31m               self.trainable_variables)\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_minimize\u001b[0;34m(strategy, tape, optimizer, loss, trainable_variables)\u001b[0m\n\u001b[1;32m   1787\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scaled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m   \u001b[0;31m# Whether to aggregate gradients outside of optimizer. This requires support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_zeros\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    662\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2678\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;31m# Go through tensor shapes to get int64-if-needed semantics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         shape = constant_op._tensor_shape_tensor_conversion_function(\n\u001b[0;32m-> 2727\u001b[0;31m             tensor_shape.TensorShape(shape))\n\u001b[0m\u001b[1;32m   2728\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[0;31m# Happens when shape is a list with tensor elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[0;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"shape_as_tensor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    304\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    305\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 306\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    307\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3312\u001b[0m     \u001b[0mnode_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_NodeDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3314\u001b[0;31m     \u001b[0minput_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3315\u001b[0m     \u001b[0mcontrol_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_dependencies_for_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3316\u001b[0m     \u001b[0;31m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucyOayELU9k4",
        "colab_type": "text"
      },
      "source": [
        "### Support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBHfROTU8dY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "6ee408ca-4e18-4e7f-fd83-b1beb8f25e6a"
      },
      "source": [
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_validation_predicted = svm.predict(X_validation)\n",
        "print(f\"Validation accuracy for SVM over pretrained model is: {np.sum(y_validation_predicted == y_validation) / y_validation.shape[0]}\")\n",
        "print(\"Validation set confusion matrix:\")\n",
        "pd.DataFrame(confusion_matrix(y_validation, y_validation_predicted), index=['True: 1 star', 'True: 5 stars'], columns=['Predicted: 1 star', 'Predicted: 5 stars'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy for SVM over pretrained model is: 0.7453703703703703\n",
            "Validation set confusion matrix:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted: 1 star</th>\n",
              "      <th>Predicted: 5 stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True: 1 star</th>\n",
              "      <td>629</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True: 5 stars</th>\n",
              "      <td>205</td>\n",
              "      <td>659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted: 1 star  Predicted: 5 stars\n",
              "True: 1 star                 629                 235\n",
              "True: 5 stars                205                 659"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvAdgEQD1M0Z",
        "colab_type": "text"
      },
      "source": [
        "SVM performed better than the neural network on the validation set. Let's see if it has the same accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASXozUgYp1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e2b6aeeb-e200-4ff2-a873-5bff620430b3"
      },
      "source": [
        "y_test_predicted = svm.predict(X_test)\n",
        "print(f\"Test accuracy for the SVM over pretrained model is: {np.sum(y_test_predicted == y_test) / y_test.shape[0]}\")\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_predicted), index=['True: 1 star', 'True: 5 stars'], columns=['Predicted: 1 star', 'Predicted: 5 stars'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for the SVM over pretrained model is: 0.7806712962962963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted: 1 star</th>\n",
              "      <th>Predicted: 5 stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True: 1 star</th>\n",
              "      <td>655</td>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True: 5 stars</th>\n",
              "      <td>170</td>\n",
              "      <td>694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Predicted: 1 star  Predicted: 5 stars\n",
              "True: 1 star                 655                 209\n",
              "True: 5 stars                170                 694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tth6Sy9gsf6j",
        "colab_type": "text"
      },
      "source": [
        "It seems like the test set accuracy is similar to the validation accuracy which probably indicates that the model has generalized pretty well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYRRLyWXAQai",
        "colab_type": "text"
      },
      "source": [
        "NLP Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ruz0EdAC4a",
        "colab_type": "text"
      },
      "source": [
        "In this section we aim to identify distinct topics discussed in the corpus of pizza review texts. Once topics are identified, reviews that do not contain food-related topics can be filtered out. This has the potential to improve the image classification in the next stage by weeding out irrelevent samples - where reviewers didn't base their scores on the pizza. \n",
        "\n",
        "This will be achieved by building a topic model that categorizes the information present in each review. A topic can be modeled as a set of words that are all related. For instance, we might say that [noise, smell, music, dirt, lighting] reflects the topic of restaurant atmosphere.  \n",
        "\n",
        "The first algorithm we will utilize is latent Dirichlet allocation. The premise of LDA (Biel et al., 2003) is that documents with similar topics use similar words. The algorithm aims to discover groups of words the occur frequently occur together in the same document. A topic is modeled as a probability distribution over words. Moreover, a document can be modeled as a probability distribution over different topics. \n",
        "\n",
        "Thus, the algorithm works as follows:\n",
        "\n",
        "1.   Remove unimportant words and set how many topics to find.\n",
        "2.   Randomly assign each word in each document to a random topic\n",
        "3.   For each document,\n",
        ">a. choose a topic, assuming all others are allocated correctly\n",
        "\n",
        ">>i. calculate the topic distribution within the document:  p(topic | document)\n",
        "\n",
        ">>ii. calculate the word distribution within the topic: p(word | topic)\n",
        "\n",
        ">> iii. multiply i and ii together and assign words to new topics based on the result\n",
        "\n",
        "4. terminate when there are no new assignments\n",
        "\n",
        "The LDA model is finetuned by several parameters:\n",
        "Alpha reflects how many topics are in a given document (higher values lead to more topics per document in the model)\n",
        "Beta reflects how many words are in a given topic (higher values lead to more words per topic in the model). In this implementation, the model is set to learn these values automatically.\n",
        "\n",
        "We compare the pure LDA model with a hybrid version that utilizes sentence embeddings of the review texts crafted by BERT (Devlin et al., 2018). Inspiration for this hybrid model comes from https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032.\n",
        "\n",
        "BERT is a pretrained language model from researchers at Google that utilizes a multiheaded transformer ANN architecture to craft word vectors that learns to capture the meaning of words from the context in which they are found. BERT and its successors have acheived impressive performance on language understatnding tasks like question answering and others. \n",
        "\n",
        "The hybrid model attempted here fuses the topic probability vector from LDA with the review text embedding from BERT to create a hybrid sample. Since the BERT vectors are much larger that the LDA vectors, scaling is performed on the LDA vectors to even out their relative importance. \n",
        "\n",
        "Clustering is then performed to distinguish different topics in the corpus (the default algorithm used here is KMeans). The most frequent words in the cluster become the topic. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uxtOU-NAdOl",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is necessary to create a universal vocabulary for the corpus. Each text is first processed at the string level and then at the word level. \n",
        "\n",
        "Examples:\n",
        "1. fix typos and missing spaces\n",
        "2. remove punctuation, capitalization, and numbers\n",
        "3. remove unimportant words (stopwords)\n",
        "4. stem words so that a fair comparison can be made (for example, salted and salty both become salt)\n",
        "\n",
        "For the sake of brevity, the preproccessing functions are contained in a seperate script located at:\n",
        "https://github.com/shaifuss/data_science_seminar/blob/master/preproccessing.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K82IjiGhCMo2",
        "colab_type": "text"
      },
      "source": [
        "In an attempt to improve the clustering for the hybrid model, dimensionality reduction is performed. An autoencoder is trained on the LDA+BERT vectors. Once training is complete, the middle hidden layer of length 32 is taken as a representation of the original data. This achieves compression of at least 25x. \n",
        "\n",
        "Clustering is performed on the compressed vectors. After clustering, topics are constructed from the top 5 most frequent words in each cluster.\n",
        "\n",
        "For the hybrid model, BERT outputs sentence-level encodings of length 768 (the first output vector corresponding to the CLS token passed in at the start of each text). An LDA vector contains a probability value in range [0.0, 1.0] for each possible topic. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQzgdG14COCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=200,\n",
        "                                        batch_size=128,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBiHU0lC8vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, method, k=4):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 250  # parameter for reletive importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + str(round(time.time(),0))\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        Get vector representations from selected methods\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'LDA':\n",
        "            print('Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20, alpha='auto', )\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            return vec\n",
        "\n",
        "        #         elif method == 'LDA_BERT':\n",
        "        else: \n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('Fitting Autoencoder ...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        Fit the topic model for selected method given the preprocessed data\n",
        "        :docs: list of documents, each doc is preprocessed as tokens\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('Fitting LDA ...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary\n",
        "                                                                , alpha='auto', eta='auto', minimum_probability=0.3)\n",
        "        else:\n",
        "            print('Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "        \"\"\"\n",
        "        Predict topics for new_documents\n",
        "        \"\"\"\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "\n",
        "        print(\"Predicting...\")\n",
        "        print(\"Out of sample is ()\".format(out_of_sample))\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":   # take the most prevalent topic\n",
        "            #lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     #key=lambda x: x[1], reverse=True)[0][0], corpus)))\n",
        "            lbs = []\n",
        "            for text in corpus:\n",
        "              lbs.append(self.ldamodel.get_document_topics(text))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs\n",
        "    def persist(self, workdir):\n",
        "      with open(os.path.join(workdir, \"test_\" + self.id), 'wb') as f:\n",
        "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjdLrj_5DGvw",
        "colab_type": "text"
      },
      "source": [
        "Coherence is used for topic model evaluation (Roder et al., 2015). As topic models produced algorithmically are sometimes not easily interpreted by humans, it is necessary to have a way to objectively measure how well the words in a topic go together. \n",
        "\n",
        "Very generally, coherence can be evaluated for a topic by first computing a Cartesian product on itself to construct pairs (excluding twin pairs). The co-occurence of each pair is checked in an external reference - a very large corpus of texts said to represent common language usage. The higher the co-occurence scores, the higher the topic coherence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDekpuykvS47",
        "colab_type": "text"
      },
      "source": [
        "We hypothesize that the following four topics are relevant to pizza reviews.\n",
        "\n",
        "1. Food\n",
        "2. Service\n",
        "3. Atmosphere\n",
        "4. Value\n",
        "\n",
        "Adding +/- 1, we train models with 3, 4, and 5 topics for each method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2urPDtaoRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(method, ntopic, sentences, token_lists, idx_in):\n",
        "  \n",
        "  print(\"Starting training...\")\n",
        "  start = time.time()\n",
        "  tm = Topic_Model(method, k = ntopic)\n",
        "  tm.fit(sentences, token_lists)\n",
        "  end = time.time()\n",
        "  print(\"Training on {} reviews took {} minutes\".format(len(sentences), str((end - start)/60)))\n",
        "  \n",
        "  return tm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQQy8uEoa3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "77a33f34-725f-4243-c743-298d52a1d339"
      },
      "source": [
        "import preproccessing\n",
        "import postprocess\n",
        "\n",
        "pic_path = 'review_photos'\n",
        "pix_review_ids = [f for f in os.listdir(pic_path) if os.path.isdir(os.path.join(pic_path, f))]\n",
        "\n",
        "text_df = pd.DataFrame.from_records(pizza_reviews)\n",
        "\n",
        "pic_review_df = text_df[text_df['review_id'].isin(pix_review_ids)]\n",
        "pic_review_df = pic_review_df.reset_index(drop=True)\n",
        "pic_review_df = pic_review_df.fillna('')\n",
        "reviews = pic_review_df.text\n",
        "\n",
        "sentences, token_lists, idx_in = preprocessing.preprocess(reviews, 5)#len(reviews) + 1)\n",
        "\n",
        "model_dict = dict()\n",
        "for method in [\"LDA\", \"LDA_BERT\"]:\n",
        "  for num_topics in range(3, 6):\n",
        "    tm = train(method, num_topics, sentences, token_lists, idx_in)\n",
        "    model_dict[method + \"_\" + str(num_topics)] = dict()\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"model\"] = tm\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"coherence\"] = get_coherence(tm, token_lists, 'c_v')\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"silhouette\"] = get_silhouette(tm)\n",
        "    if method == \"LDA\":\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = get_topic_words_lda(tm)\n",
        "    else:\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = get_topic_words_hybrid(token_lists, tm.cluster_model.labels_, k=None)\n",
        "      postprocess.visualize(tm)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1ee7d8810e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreproccessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpic_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'review_photos'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpix_review_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'postprocess'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph1S4BbBbGEJ",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "The models were run multiple times with a range of values for each hyperparameter.\n",
        "\n",
        "Observations:\n",
        "1. Coherence was consistently an order of magnitude higher for the pure LDA model. \n",
        "2. Silhoutte scores improved as the LDA vectors were weighted more heavily in the hybrid model\n",
        "3. The pure LDA model produced higher quality topics as judged by two human evaluators (us). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR5CIIYosI7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_pizza_vocab = preprocessing.stem_words([\"crust\", \"slice\", \"sauce\", \"topping\", \"cheese\", \"tomato\", \"food\", \"pie\"])\n",
        "model_dict = postproccess.get_scores(model_dict, stemmed_pizza_vocab)\n",
        "best_method = max(model_dict, key=lambda v: model_dict[v]['coherence'])\n",
        "best_topic = max(range(len(model_dict[best_method][\"scores\"])), key=model_dict[best_method][\"scores\"].__getitem__)\n",
        "print(\"best method is {}\".format(best_method))\n",
        "print(\"best_topic is at index {}\".format(best_topic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P02iiJwSQWYI",
        "colab_type": "text"
      },
      "source": [
        "For prediction with the pure LDA model, the topics are retrieved for a given review. If the best pizza topic is not the most prominent topic in that review, our hypothesis is that discarding it will lead to improved performance for the image classifier.\n",
        "\n",
        "For prediction with the LDA+BERT version, a review is place into a cluster with the pretrained model. Similar to the pure model, we should keep the reviews that fall into the best pizza topic cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZNfuv3QVgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "print(\"There are {} reviews before filtering\".format(len(sentences)))\n",
        "topic_lists = model_dict[best_method][\"model\"].predict(sentences, token_lists, True)\n",
        "indicies = []\n",
        "print(topic_lists)\n",
        "for i, topic_list in enumerate(topic_lists):\n",
        "  topics = [x[0] for x in topic_list]\n",
        "  if best_topic in topics:\n",
        "    indicies.append(i)\n",
        "print(\"There are {} reviews afer filtering\".format(len(indicies)))\n",
        "\n",
        "food_based_reviews = pic_review_df[pic_review_df.index.isin(indicies)]\n",
        "print(\"Example reviews that passed the filter:\\n\")\n",
        "print(food_based_reviews.loc[random.choice(indicies), \"text\"])\n",
        "print(\"---------------------------------------------------------------\")\n",
        "print(food_based_reviews.loc[random.choice(indicies), \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}