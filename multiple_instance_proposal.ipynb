{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiple_instance_proposal",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17_7A_RSM9FmL3NvQT0MFs_JYEIqWgEzM",
      "authorship_tag": "ABX9TyM6BdLVfAw0/zSwSxhLSDWY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaifuss/data_science_seminar/blob/master/multiple_instance_proposal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ERhmyil56x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, ConvLSTM2D, Reshape, BatchNormalization, Conv3D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import Input, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "BAG_SIZE = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1luMYw-6xRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c85aef3c-5846-44c7-fe16-509aefe2823f"
      },
      "source": [
        "# load stuff\n",
        "review_pix_dir = r'/content/drive/My Drive/Data Science Class/review_photos2/review_photos'\n",
        "review_ids_with_photos = list(os.listdir(review_pix_dir))\n",
        "with open(r'/content/drive/My Drive/Data Science Class/pizza_reviews.json', 'r') as f:\n",
        "    pizza_reviews = json.load(f)\n",
        "\n",
        "pizza_reviews_by_id = {pizza_review[\"review_id\"]: pizza_review for pizza_review in pizza_reviews}\n",
        "print(\"There are {} candidate reviews\".format(len(review_ids_with_photos)))\n",
        "# filter out review ids that don't have enough pictures\n",
        "for review_id in os.listdir(review_pix_dir):\n",
        "  pixdir = os.path.join(review_pix_dir, review_id)\n",
        "  subdir = os.path.join(review_pix_dir, pixdir)\n",
        "  if len(os.listdir(subdir)) < BAG_SIZE:\n",
        "    review_ids_with_photos.remove(review_id)\n",
        "print(\"Of those, {} have enough pictures\".format(len(review_ids_with_photos)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 647 candidate reviews\n",
            "Of those, 173 have enough pictures\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-crJVQZRqhCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ef9a1bb-ce8b-4d22-ef10-a421af927aca"
      },
      "source": [
        "\n",
        "\n",
        "stars_to_photos = defaultdict(lambda: [])\n",
        "bag_counter = 0\n",
        "for review_id_with_photos in review_ids_with_photos:\n",
        "    review_path = os.path.join(review_pix_dir, review_id_with_photos)\n",
        "    image_bag = []  # will produce multiple bags per review\n",
        "    for i, filename in enumerate(os.listdir(review_path)):\n",
        "        img_path = os.path.join(review_path, filename)\n",
        "        img = image.load_img(img_path, target_size=(299, 299))\n",
        "        img_array = image.img_to_array(img)\n",
        "        x = np.expand_dims(img_array, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "        \n",
        "        image_bag.append(x)\n",
        "        #\n",
        "        if i == BAG_SIZE - 1:\n",
        "          bag_counter += 1\n",
        "          t = np.stack(image_bag, axis=0)\n",
        "          stars_to_photos[pizza_reviews_by_id[review_id_with_photos][\"stars\"]].append(t)\n",
        "          image_bag = []\n",
        "print(\"Created {} bags of size {}\".format(bag_counter, BAG_SIZE))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created 173 bags of size 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-gYT7LnmKNe",
        "colab_type": "code",
        "outputId": "d7318046-f450-455e-efc5-564ccbbb990b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "#from keras.layers.convolutional import Conv3D\n",
        "#from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "# We create a layer which take as input movies of shape\n",
        "# (n_frames, width, height, channels) and returns a movie\n",
        "# of identical shape.\n",
        "\n",
        "seq = Sequential()\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   input_shape=(BAG_SIZE, 299, 299, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
        "               activation='sigmoid',\n",
        "               padding='same', data_format='channels_last'))\n",
        "seq.add(Reshape((-1, 1)))\n",
        "seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
        "seq.summary()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_lst_m2d_24 (ConvLSTM2D) (None, 2, 299, 299, 40)   62080     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 2, 299, 299, 40)   160       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_25 (ConvLSTM2D) (None, 2, 299, 299, 40)   115360    \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 2, 299, 299, 40)   160       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_26 (ConvLSTM2D) (None, 2, 299, 299, 40)   115360    \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 2, 299, 299, 40)   160       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_27 (ConvLSTM2D) (None, 2, 299, 299, 40)   115360    \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 2, 299, 299, 40)   160       \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 2, 299, 299, 1)    1081      \n",
            "_________________________________________________________________\n",
            "reshape_6 (Reshape)          (None, None, 1)           0         \n",
            "=================================================================\n",
            "Total params: 409,881\n",
            "Trainable params: 409,561\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1xeUgYPanX",
        "colab_type": "code",
        "outputId": "24f29c75-336f-4a84-c1f8-f958c4406560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "one_star = np.rollaxis(np.concatenate(stars_to_photos[1], axis=1),1)\n",
        "two_star = np.rollaxis(np.concatenate(stars_to_photos[2], axis=1), 1)\n",
        "five_star = np.rollaxis(np.concatenate(stars_to_photos[5], axis=1), 1)\n",
        "\n",
        "X = np.concatenate([one_star] + [two_star])\n",
        "X = np.concatenate([X] + [five_star])\n",
        "y = np.repeat(0.0, len(stars_to_photos[1]) + len(stars_to_photos[2]))\n",
        "y = np.concatenate([y, np.repeat(1.0, len(stars_to_photos[5]))]).reshape(-1, 1)\n",
        "print(\"X dimensions: {}, y dimensions {}\".format(X.shape, y.shape))\n",
        "print(\"{} examples were created\".format(X.shape[0]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X dimensions: (107, 2, 299, 299, 3), y dimensions (107, 1)\n",
            "107 examples were created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBnhQq7mP23r",
        "colab_type": "code",
        "outputId": "16822b7f-e2bf-4719-e73d-23f8872bd37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((74, 2, 299, 299, 3), (74, 1), (33, 2, 299, 299, 3), (33, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qhBXg2gP5Z9",
        "colab_type": "code",
        "outputId": "ae3c922d-0577-4e1c-d9c8-d67fd0ee64be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "seq.fit(X_train, y_train, batch_size=5, validation_split=0.3, epochs=50, callbacks=[EarlyStopping(patience=15, restore_best_weights=True)])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "11/11 [==============================] - 41s 4s/step - loss: 0.8101 - val_loss: 0.6931\n",
            "Epoch 2/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.8156 - val_loss: 0.6930\n",
            "Epoch 3/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.8120 - val_loss: 0.6929\n",
            "Epoch 4/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7948 - val_loss: 0.6928\n",
            "Epoch 5/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7909 - val_loss: 0.6927\n",
            "Epoch 6/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7819 - val_loss: 0.6926\n",
            "Epoch 7/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7723 - val_loss: 0.6925\n",
            "Epoch 8/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7796 - val_loss: 0.6924\n",
            "Epoch 9/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7729 - val_loss: 0.6924\n",
            "Epoch 10/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7586 - val_loss: 0.6922\n",
            "Epoch 11/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7561 - val_loss: 0.6921\n",
            "Epoch 12/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7502 - val_loss: 0.6920\n",
            "Epoch 13/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7594 - val_loss: 0.6919\n",
            "Epoch 14/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7480 - val_loss: 0.6918\n",
            "Epoch 15/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7503 - val_loss: 0.6916\n",
            "Epoch 16/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7385 - val_loss: 0.6915\n",
            "Epoch 17/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7428 - val_loss: 0.6913\n",
            "Epoch 18/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7406 - val_loss: 0.6912\n",
            "Epoch 19/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7332 - val_loss: 0.6910\n",
            "Epoch 20/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7285 - val_loss: 0.6908\n",
            "Epoch 21/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7287 - val_loss: 0.6906\n",
            "Epoch 22/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7211 - val_loss: 0.6905\n",
            "Epoch 23/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7289 - val_loss: 0.6902\n",
            "Epoch 24/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7214 - val_loss: 0.6900\n",
            "Epoch 25/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7302 - val_loss: 0.6898\n",
            "Epoch 26/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7229 - val_loss: 0.6896\n",
            "Epoch 27/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7098 - val_loss: 0.6893\n",
            "Epoch 28/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7152 - val_loss: 0.6891\n",
            "Epoch 29/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7113 - val_loss: 0.6889\n",
            "Epoch 30/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7118 - val_loss: 0.6887\n",
            "Epoch 31/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7204 - val_loss: 0.6885\n",
            "Epoch 32/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7115 - val_loss: 0.6883\n",
            "Epoch 33/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7158 - val_loss: 0.6881\n",
            "Epoch 34/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7093 - val_loss: 0.6881\n",
            "Epoch 35/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7006 - val_loss: 0.6880\n",
            "Epoch 36/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7145 - val_loss: 0.6880\n",
            "Epoch 37/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7046 - val_loss: 0.6882\n",
            "Epoch 38/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7003 - val_loss: 0.6884\n",
            "Epoch 39/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7129 - val_loss: 0.6887\n",
            "Epoch 40/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7062 - val_loss: 0.6893\n",
            "Epoch 41/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.6942 - val_loss: 0.6900\n",
            "Epoch 42/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7142 - val_loss: 0.6909\n",
            "Epoch 43/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.6997 - val_loss: 0.6919\n",
            "Epoch 44/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.6967 - val_loss: 0.6934\n",
            "Epoch 45/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.6924 - val_loss: 0.6950\n",
            "Epoch 46/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7173 - val_loss: 0.6970\n",
            "Epoch 47/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7016 - val_loss: 0.6989\n",
            "Epoch 48/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7085 - val_loss: 0.7011\n",
            "Epoch 49/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.7022 - val_loss: 0.7034\n",
            "Epoch 50/50\n",
            "11/11 [==============================] - 40s 4s/step - loss: 0.6958 - val_loss: 0.7060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f73ab204cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLUb79K4xGpV",
        "colab_type": "code",
        "outputId": "1d484d7a-f6a3-4cfd-8e86-8595ff3a9cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "seq.evaluate(X_test, y_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 116ms/step - loss: 0.6880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6880099773406982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}