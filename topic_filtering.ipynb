{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_filtering.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/shaifuss/data_science_seminar/blob/master/topic_filtering.ipynb",
      "authorship_tag": "ABX9TyOgjoUdCYz1QI59Wbbme8L3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2cf83a1927ce44eeb14ddf2286e747bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26aa133982944e6c866b5495b4e6f83f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6f14832a82704b7281a20fd38ba72099",
              "IPY_MODEL_1bd28eac61db4743b40601824b0866fd"
            ]
          }
        },
        "26aa133982944e6c866b5495b4e6f83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f14832a82704b7281a20fd38ba72099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f773951407f4888bad153604126da76",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7367,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7367,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5bff555f4f14902abd5e70f1b0f6198"
          }
        },
        "1bd28eac61db4743b40601824b0866fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84e3ff9dadf2489ea0a42e66f9edb3b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7367/7367 [12:42&lt;00:00,  9.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40a71767577d4dda8dd2b35e00e4dde8"
          }
        },
        "3f773951407f4888bad153604126da76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5bff555f4f14902abd5e70f1b0f6198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84e3ff9dadf2489ea0a42e66f9edb3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40a71767577d4dda8dd2b35e00e4dde8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a17d0962c1f4078b69e9276bf5b1ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3c2131523fd14cfcae03c1e8f44dd52e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d5575eddbd0843b88d37ece4078ec04f",
              "IPY_MODEL_002dde428ef1464caf2314ad5b9103bc"
            ]
          }
        },
        "3c2131523fd14cfcae03c1e8f44dd52e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5575eddbd0843b88d37ece4078ec04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_48dc9d3ae137446683aa8115b0e8d40c",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7367,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7367,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d3c4444b4674131b6a364ae2688e83a"
          }
        },
        "002dde428ef1464caf2314ad5b9103bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ccd4016af3714e43af73be2718bb8a5e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7367/7367 [12:33&lt;00:00,  9.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4ecf4355f8b4e0aa01b2fedff43bb78"
          }
        },
        "48dc9d3ae137446683aa8115b0e8d40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d3c4444b4674131b6a364ae2688e83a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ccd4016af3714e43af73be2718bb8a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4ecf4355f8b4e0aa01b2fedff43bb78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16ceaf540c724a6da71d65059aa4cdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d492f6988f6a4986973fa6b898cb574e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4d2d8cceb3fa46728170dcb63cd96c1a",
              "IPY_MODEL_040b48baa670451eacd6a63c9d9cd5b8"
            ]
          }
        },
        "d492f6988f6a4986973fa6b898cb574e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d2d8cceb3fa46728170dcb63cd96c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d48c26305b5f44ee98b0ee927967f20d",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7367,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7367,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_776e0c7cb87645fd9f5c87443292f067"
          }
        },
        "040b48baa670451eacd6a63c9d9cd5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ed8a06a7a934000a50089780b466566",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7367/7367 [12:35&lt;00:00,  9.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e3c6be5d138437b9e55b7845234564a"
          }
        },
        "d48c26305b5f44ee98b0ee927967f20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "776e0c7cb87645fd9f5c87443292f067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ed8a06a7a934000a50089780b466566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e3c6be5d138437b9e55b7845234564a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "485db6fe52c541e78a154c5ee13554d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c113d259a92547ddb84052948b4dca4e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bd9cbaf00df34a7f80753a80c3d4fa3f",
              "IPY_MODEL_56dcc13f9c1b40e3b8d8b8f43670381b"
            ]
          }
        },
        "c113d259a92547ddb84052948b4dca4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd9cbaf00df34a7f80753a80c3d4fa3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_714cdfd9821f489aa24eb526ea6d9a76",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1288,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1288,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3845ad38b68344c48833c2fd5079d3c3"
          }
        },
        "56dcc13f9c1b40e3b8d8b8f43670381b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13929f40712b419cb6eda17658317152",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1288/1288 [05:42&lt;00:00,  3.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e23ac6d22c0a467d948ecc00c7c55b68"
          }
        },
        "714cdfd9821f489aa24eb526ea6d9a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3845ad38b68344c48833c2fd5079d3c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13929f40712b419cb6eda17658317152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e23ac6d22c0a467d948ecc00c7c55b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaifuss/data_science_seminar/blob/master/topic_filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vPFvziHYMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "415adc8a-1865-4de5-e6cd-f30e94a44999"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "!pip install pyspellchecker -q\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "workdir = r'/content/drive/My Drive/Data Science Class'\n",
        "PERSIST_INTERVAL = 30 # in minutes\n",
        "\n",
        "if not os.path.exists('review_photos'):\n",
        "    if os.path.exists('data_science_seminar'):\n",
        "        %cd data_science_seminar\n",
        "    else:\n",
        "        !git clone https://github.com/shaifuss/data_science_seminar.git\n",
        "        %cd data_science_seminar\n",
        "if (not os.path.exists('yelp_academic_dataset_business.json')) or (not os.path.exists('yelp_academic_dataset_review.json')):\n",
        "    kaggle_path = os.path.expanduser('~/.kaggle')\n",
        "    kaggle_json_path = os.path.join(kaggle_path, 'kaggle.json')\n",
        "    if not os.path.exists(kaggle_json_path):\n",
        "        from getpass import getpass\n",
        "        kaggle_json = getpass('Insert kaggle.json:')\n",
        "        os.makedirs(kaggle_path, exist_ok=True)\n",
        "        with open(kaggle_json_path, 'w') as f:\n",
        "            f.write(kaggle_json)\n",
        "        os.chmod(kaggle_json_path, 0o600)\n",
        "    !kaggle datasets download yelp-dataset/yelp-dataset\n",
        "    !unzip yelp-dataset.zip yelp_academic_dataset_business.json yelp_academic_dataset_review.json\n",
        "    !rm yelp-dataset.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9MB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 778kB 21.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 37.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 41.4MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'data_science_seminar'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 23658 (delta 8), reused 0 (delta 0), pack-reused 23643\u001b[K\n",
            "Receiving objects: 100% (23658/23658), 1.66 GiB | 28.55 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n",
            "Checking out files: 100% (14961/14961), done.\n",
            "/content/data_science_seminar\n",
            "Insert kaggle.json:··········\n",
            "Downloading yelp-dataset.zip to /content/data_science_seminar\n",
            "100% 4.47G/4.48G [01:30<00:00, 47.1MB/s]\n",
            "100% 4.48G/4.48G [01:30<00:00, 53.2MB/s]\n",
            "Archive:  yelp-dataset.zip\n",
            "  inflating: yelp_academic_dataset_business.json  \n",
            "  inflating: yelp_academic_dataset_review.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyS9X2QrgrTF",
        "colab_type": "text"
      },
      "source": [
        "In this section we aim to identify distinct topics discussed in the corpus of pizza review texts. Once topics are identified, reviews that do not contain food-related topics can be filtered out. This has the potential to improve the image classification in the next stage by weeding out irrelevent samples - where reviewers didn't base their scores on the pizza. \n",
        "\n",
        "This will be achieved by building a topic model that categorizes the information present in each review. A topic can be modeled as a set of words that are all related. For instance, we might say that [noise, smell, music, dirt, lighting] reflects the topic of restaurant atmosphere.  \n",
        "\n",
        "The first algorithm we will utilize is latent Dirichlet allocation. The premise of LDA (Biel et al., 2003) is that documents with similar topics use similar words. The algorithm aims to discover groups of words the occur frequently occur together in the same document. A topic is modeled as a probability distribution over words. Moreover, a document can be modeled as a probability distribution over different topics. \n",
        "\n",
        "Thus, the algorithm works as follows:\n",
        "\n",
        "1.   Remove unimportant words and set how many topics to find.\n",
        "2.   Randomly assign each word in each document to a random topic\n",
        "3.   For each document,\n",
        ">a. choose a topic, assuming all others are allocated correctly\n",
        "\n",
        ">>i. calculate the topic distribution within the document:  p(topic | document)\n",
        "\n",
        ">>ii. calculate the word distribution within the topic: p(word | topic)\n",
        "\n",
        ">> iii. multiply i and ii together and assign words to new topics based on the result\n",
        "\n",
        "4. terminate when there are no new assignments\n",
        "\n",
        "The LDA model is finetuned by several parameters:\n",
        "Alpha reflects how many topics are in a given document (higher values lead to more topics per document in the model)\n",
        "Beta reflects how many words are in a given topic (higher values lead to more words per topic in the model). In this implementation, the model is set to learn these values automatically.\n",
        "\n",
        "We compare the pure LDA model with a hybrid version that utilizes sentence embeddings of the review texts crafted by BERT (Devlin et al., 2018). Inspiration for this hybrid model comes from https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032.\n",
        "\n",
        "BERT is a pretrained language model from researchers at Google that utilizes a multiheaded transformer ANN architecture to craft word vectors that learns to capture the meaning of words from the context in which they are found. BERT and its successors have acheived impressive performance on language understatnding tasks like question answering and others. \n",
        "\n",
        "The hybrid model attempted here fuses the topic probability vector from LDA with the review text embedding from BERT to create a hybrid sample. Clustering is then performed to distinguish different topics in the corpus. The most frequent words in the cluster become the topic. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfst91OkIcNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "254de74b-509c-4312-dee7-407c85fed5ef"
      },
      "source": [
        "# loading reviews into dataframe and peak at the data\n",
        "def load_reviews():\n",
        "  with open(r'/content/drive/My Drive/Data Science Class/pizza_reviews.json', 'r') as f:\n",
        "    pizza_reviews = json.load(f)\n",
        "  return pizza_reviews\n",
        "review_list = load_reviews()\n",
        "print(\"Total pizzaria reviews: {}\".format(len(review_list)))\n",
        "review_df = pd.DataFrame(review_list)\n",
        "\n",
        "text_df = review_df[['review_id', 'text']].copy()\n",
        "text_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total pizzaria reviews: 479792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mM8i91yWP1QbImEvz5ds0w</td>\n",
              "      <td>In the heart of Chinatown, I discovered it enr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>09qxjFi4abaW66JeSLazuQ</td>\n",
              "      <td>Was a Chicago style deep dish.  Homemade type ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>K-wdPGHbErfxbKK6PetrmA</td>\n",
              "      <td>First time eating there and everything was so ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jkVxX4ieJwVRO9n4E8tNMw</td>\n",
              "      <td>More than just  Pizza. This location is small ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lb9r62Qlu12ZB909CbFeOQ</td>\n",
              "      <td>I ordered a pizza at 4:49. Got an email that s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id                                               text\n",
              "0  mM8i91yWP1QbImEvz5ds0w  In the heart of Chinatown, I discovered it enr...\n",
              "1  09qxjFi4abaW66JeSLazuQ  Was a Chicago style deep dish.  Homemade type ...\n",
              "2  K-wdPGHbErfxbKK6PetrmA  First time eating there and everything was so ...\n",
              "3  jkVxX4ieJwVRO9n4E8tNMw  More than just  Pizza. This location is small ...\n",
              "4  Lb9r62Qlu12ZB909CbFeOQ  I ordered a pizza at 4:49. Got an email that s..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di_BNmLcN5Bv",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is necessary to create a universal vocabulary for the corpus. Each text is first processed at the string level and then at the word level. \n",
        "\n",
        "Examples:\n",
        "1. fix typos and missing spaces\n",
        "2. remove punctuation, capitalization, and numbers\n",
        "3. remove unimportant words (stopwords)\n",
        "4. stem words so that a fair comparison can be made (for example, salted and salty both become salt)\n",
        "\n",
        "Due to constraints on time and computational resources, a sample of 58993 pizza reviews was used out of nearly 480000 in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkiPLh-NzPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "20c5438d-1870-406e-bb71-cc246320d295"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f69KGrPSNRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regex_filter(sentence):\n",
        "    # fix missing delimiter - i.e deepDishPizza\n",
        "    sentence = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'&gt|&lt', ' ', sentence)\n",
        "    # fix letter repetition (if more than 2)\n",
        "    sentence = re.sub(r'([a-z])\\1{2,}', r'\\1', sentence)\n",
        "    # fix non-word repetition (if more than 1)\n",
        "    sentence = re.sub(r'([\\W+])\\1{1,}', r'\\1', sentence)\n",
        "    # string * as delimiter\n",
        "    sentence = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', sentence)\n",
        "    # xxx[?!]. -- > xxx.\n",
        "    sentence = re.sub(r'\\W+?\\.', '.', sentence)\n",
        "    # [.?!] --> [.?!] xxx\n",
        "    sentence = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', sentence)\n",
        "    # fix phrase repetition\n",
        "    sentence = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', sentence)\n",
        "\n",
        "    return sentence.strip()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bljAxTQuNLZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove numbers and punctuation marks\n",
        "def filter_punctuation(word_list):\n",
        "    return [word for word in word_list if word.isalpha()]\n",
        "\n",
        "# remove unimportant connective words such as \"and\", \"the\", etc\n",
        "def filter_stopwords(word_list):\n",
        "  return [word for word in word_list if word not in stopwords.words('english')]\n",
        "\n",
        "# keep only nouns\n",
        "def retain_nouns(word_list):\n",
        "    return [word for (word, pos) in nltk.pos_tag(word_list) if pos[:2] in ['NN']]\n",
        "\n",
        "# normlize for part of speech\n",
        "def stem_words(word_list):\n",
        "  ps = PorterStemmer()\n",
        "  return [ps.stem(word) for word in word_list]\n",
        "\n",
        "def fix_spelling(word_list):\n",
        "  spell = SpellChecker()\n",
        "  return [spell.correction(word) for word in word_list]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23SUE1RhVN83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_words(text):\n",
        "  word_list = word_tokenize(text)\n",
        "  word_list = filter_punctuation(word_list)\n",
        "  word_list = fix_spelling(word_list) \n",
        "  word_list = filter_stopwords(word_list)\n",
        "  word_list = retain_nouns(word_list)\n",
        "  return stem_words(word_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94WRVmMGhyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def persist_processed_data(texts, token_lists, idx_in):\n",
        "  print(\"Saving at {}\".format(time.time()))\n",
        "  with open(os.path.join(workdir, processed_file), 'wb') as p:\n",
        "          pickle.dump([texts, token_lists, idx_in], p)   "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmw52otQ4cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(reviews, samp_size=None):\n",
        "  if not samp_size:\n",
        "        samp_size = 1000\n",
        "\n",
        "  start = time.time()\n",
        "  print('Stage 1: Preprocess raw review texts')\n",
        "  texts = []  \n",
        "  token_lists = []  \n",
        "  idx_in = []\n",
        "  batch_start = time.time()\n",
        "  indicies = np.random.choice(len(reviews), samp_size)\n",
        "  for i in indicies:\n",
        "      text = regex_filter(reviews[i])\n",
        "      token_list = preprocess_words(text)\n",
        "      if token_list:\n",
        "        idx_in.append(i)\n",
        "        texts.append(text)\n",
        "        token_lists.append(token_list)\n",
        "      batch_end = time.time() \n",
        "      if ((batch_end - batch_start)/60) > PERSIST_INTERVAL:\n",
        "        batch_start = batch_end\n",
        "        persist_processed_data(texts, token_lists, idx_in)\n",
        "           \n",
        "  end = time.time()\n",
        "  print(\"Preprocessing {} reviews took {} minutes\".format(len(indicies), str((end - start)/60)))\n",
        "  persist_processed_data(texts, token_lists, idx_in)\n",
        "  return texts, token_lists, idx_in\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXvFzyNtN0gr",
        "colab_type": "text"
      },
      "source": [
        "For the hybrid model, BERT outputs sentence-level encodings of length 768 (the first output vector corresponding to the CLS token passed in at the start of each text). An LDA vector contains a probability value in range [0.0, 1.0] for each possible topic. The LDA and BERT outputs are concatenated together and clustering is performed on the resulting high-dimension vectors (the default algorithm used here is KMeans). Concatenization is parametrized by gamma, the factor to scale the LDA vectors so as to increase their relative importance. \n",
        "\n",
        "In an attempt to improve the clustering, dimensionality reduction is performed. An autoencoder is trained on the LDA+BERT vectors. Once training is complete, the middle hidden layer of length 32 is taken as a representation of the original data. This achieves compression of at least 25x. \n",
        "\n",
        "Clustering is performed on the compressed vectors. After clustering, topics are constructed from the top 5 most frequent words in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_Yl6tIaO2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19c66d2a-4e31-442a-dccd-ee9763e52de7"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=200,\n",
        "                                        batch_size=128,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIiZR7zi4rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, method, k=4):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 250  # parameter for reletive importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + str(round(time.time(),0))\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        Get vector representations from selected methods\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'LDA':\n",
        "            print('Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20, alpha='auto', )\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            return vec\n",
        "\n",
        "        #         elif method == 'LDA_BERT':\n",
        "        else: \n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('Fitting Autoencoder ...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        Fit the topic model for selected method given the preprocessed data\n",
        "        :docs: list of documents, each doc is preprocessed as tokens\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('Fitting LDA ...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary\n",
        "                                                                , alpha='auto', eta='auto', minimum_probability=0.3)\n",
        "        else:\n",
        "            print('Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=True):\n",
        "        \"\"\"\n",
        "        Predict topics for new_documents\n",
        "        \"\"\"\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "\n",
        "        print(\"Predicting...\") #; ipdb.set_trace()\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":   # take the most prevalent topic\n",
        "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
        "                                    corpus)))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs\n",
        "    def persist(self, workdir):\n",
        "      with open(os.path.join(workdir, \"test_\" + self.id), 'wb') as f:\n",
        "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_LG2cBV8Li",
        "colab_type": "text"
      },
      "source": [
        "Coherence is used for topic model evaluation (Roder et al., 2015). As topic models produced algorithmically are sometimes not easily interpreted by humans, it is necessary to have a way to objectively measure how well the words in a topic go together. \n",
        "\n",
        "Very generally, coherence can be evaluated for a topic by first computing a Cartesian product on itself to construct pairs (excluding twin pairs). The co-occurence of each pair is checked in an external reference - a very large corpus of texts said to represent common language usage. The higher the co-occurence scores, the higher the topic coherence.\n",
        "\n",
        "A silhoutte score is used to evaluate clustering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsan4epLeHG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.metrics import silhouette_score\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import os\n",
        "\n",
        "\n",
        "def get_topic_words_hybrid(token_lists, labels, k=None):\n",
        "    \"\"\"\n",
        "    get top words within each topic from clustering results\n",
        "    \"\"\"\n",
        "    if k is None:\n",
        "        k = len(np.unique(labels))\n",
        "    topics = ['' for _ in range(k)]\n",
        "    for i, c in enumerate(token_lists):\n",
        "        topics[labels[i]] += (' ' + ' '.join(c))\n",
        "    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
        "    # get sorted word counts\n",
        "    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
        "    # get topics\n",
        "    topics = list(map(lambda x: list(map(lambda x: x[0], x[:5])), word_counts))\n",
        "    return topics\n",
        "\n",
        "def get_topic_words_lda(model):\n",
        "    return CoherenceModel.top_topics_as_word_lists(model=model.ldamodel, dictionary=model.dictionary, topn=5)\n",
        "\n",
        "def get_coherence(model, token_lists, measure='c_v'):\n",
        "    \"\"\"\n",
        "    Get model coherence from gensim.models.coherencemodel\n",
        "    :param model: Topic_Model object\n",
        "    :param token_lists: token lists of docs\n",
        "    :param topics: topics as top words\n",
        "    :param measure: coherence metrics\n",
        "    :return: coherence score\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
        "                            coherence=measure)\n",
        "        print(CoherenceModel.top_topics_as_word_lists(model=model.ldamodel, dictionary=model.dictionary, topn=5))\n",
        "    else:\n",
        "        topics = get_topic_words_hybrid(token_lists, model.cluster_model.labels_)\n",
        "        print(\"Topics are:\\n{}\".format(topics))\n",
        "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
        "                            coherence=measure)\n",
        "    return cm.get_coherence()\n",
        "\n",
        "def get_silhouette(model):\n",
        "    \"\"\"\n",
        "    Get silhouette score from model\n",
        "    :param model: Topic_Model object\n",
        "    :return: silhouette score\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        return\n",
        "    lbs = model.cluster_model.labels_\n",
        "    vec = model.vec[model.method]\n",
        "    return silhouette_score(vec, lbs)\n",
        "\n",
        "def plot_proj(embedding, lbs):\n",
        "    \"\"\"\n",
        "    Plot UMAP embeddings\n",
        "    :param embedding: UMAP (or other) embeddings\n",
        "    :param lbs: labels\n",
        "    \"\"\"\n",
        "    n = len(embedding)\n",
        "    counter = Counter(lbs)\n",
        "    for i in range(len(np.unique(lbs))):\n",
        "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
        "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YNL6n9hkNoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize(model):\n",
        "    \"\"\"\n",
        "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
        "    :param model: Topic_Model object\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        return\n",
        "    reducer = umap.UMAP()\n",
        "    print('Calculating UMAP projection ...')\n",
        "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
        "    plot_proj(vec_umap, model.cluster_model.labels_)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDekpuykvS47",
        "colab_type": "text"
      },
      "source": [
        "We hypothesize that the following four topics are relevant to pizza reviews.\n",
        "\n",
        "1. Food\n",
        "2. Service\n",
        "3. Atmosphere\n",
        "4. Value\n",
        "\n",
        "Adding +/- 1 to the assumed number of topics, we train 3 models for each method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2urPDtaoRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(method, ntopic, sentences, token_lists, idx_in):\n",
        "    \n",
        "  #processed_file = r'topic_data/processed_58933.pkl'\n",
        "  tmfile =  method + \"_\" + str(round(time.time(),0)) +'.file'\n",
        "  \n",
        "  #with open(os.path.join(workdir, processed_file), 'rb') as p:\n",
        "  #  sentences, token_lists, idx_in = pickle.load(p)\n",
        "\n",
        "  print(\"Starting training\")\n",
        "  start = time.time()\n",
        "  tm = Topic_Model(method, k = ntopic)\n",
        "  tm.fit(sentences, token_lists)\n",
        "  end = time.time()\n",
        "  tm.persist(workdir)\n",
        "  print(\"Training on {} reviews took {} minutes\".format(len(sentences), str((end - start)/60)))\n",
        "  with open(os.path.join(workdir, tmfile), 'wb') as f:\n",
        "    pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n",
        "  \n",
        "  # coherence measures internal consistency of a topic\n",
        "  #print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
        "  # silhoutte measures consistency of clusters\n",
        "  #print('Silhouette Score:', get_silhouette(tm))\n",
        "  # visualize and save img\n",
        "  #visualize(tm)\n",
        "  return tm\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQQy8uEoa3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2cf83a1927ce44eeb14ddf2286e747bc",
            "26aa133982944e6c866b5495b4e6f83f",
            "6f14832a82704b7281a20fd38ba72099",
            "1bd28eac61db4743b40601824b0866fd",
            "3f773951407f4888bad153604126da76",
            "d5bff555f4f14902abd5e70f1b0f6198",
            "84e3ff9dadf2489ea0a42e66f9edb3b5",
            "40a71767577d4dda8dd2b35e00e4dde8",
            "4a17d0962c1f4078b69e9276bf5b1ff7",
            "3c2131523fd14cfcae03c1e8f44dd52e",
            "d5575eddbd0843b88d37ece4078ec04f",
            "002dde428ef1464caf2314ad5b9103bc",
            "48dc9d3ae137446683aa8115b0e8d40c",
            "9d3c4444b4674131b6a364ae2688e83a",
            "ccd4016af3714e43af73be2718bb8a5e",
            "a4ecf4355f8b4e0aa01b2fedff43bb78",
            "16ceaf540c724a6da71d65059aa4cdec",
            "d492f6988f6a4986973fa6b898cb574e",
            "4d2d8cceb3fa46728170dcb63cd96c1a",
            "040b48baa670451eacd6a63c9d9cd5b8",
            "d48c26305b5f44ee98b0ee927967f20d",
            "776e0c7cb87645fd9f5c87443292f067",
            "3ed8a06a7a934000a50089780b466566",
            "6e3c6be5d138437b9e55b7845234564a"
          ]
        },
        "outputId": "f0c32626-643d-4959-9346-d506639497e1"
      },
      "source": [
        "model_dict = dict()\n",
        "processed_file = r'topic_data/processed_58933.pkl'\n",
        "with open(os.path.join(workdir, processed_file), 'rb') as p:\n",
        "    sentences, token_lists, idx_in = pickle.load(p)\n",
        "\n",
        "for method in [\"LDA\", \"LDA_BERT\"]:\n",
        "  for num_topics in range(3, 6):\n",
        "    tm = train(method, num_topics, sentences, token_lists, idx_in)\n",
        "    model_dict[method + \"_\" + str(num_topics)] = dict()\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"model\"] = tm\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"coherence\"] = get_coherence(tm, token_lists, 'c_v')\n",
        "    model_dict[method + \"_\" + str(num_topics)][\"silhouette\"] = get_silhouette(tm)\n",
        "    if method == \"LDA\":\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = get_topic_words_lda(tm)\n",
        "    else:\n",
        "      model_dict[method + \"_\" + str(num_topics)][\"topics\"] = get_topic_words_hybrid(token_lists, tm.cluster_model.labels_, k=None)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training\n",
            "Fitting LDA ...\n",
            "Fitting LDA Done!\n",
            "Training on 58933 reviews took 0.5117189168930054 minutes\n",
            "[['food', 'place', 'servic', 'restaur', 'time'], ['pizza', 'sauc', 'slice', 'crust', 'tomato'], ['pizza', 'place', 'order', 'time', 'food']]\n",
            "Starting training\n",
            "Fitting LDA ...\n",
            "Fitting LDA Done!\n",
            "Training on 58933 reviews took 0.5330947597821554 minutes\n",
            "[['pizza', 'place', 'crust', 'slice', 'top'], ['food', 'place', 'servic', 'time', 'restaur'], ['sauc', 'pasta', 'time', 'food', 'salad'], ['order', 'pizza', 'time', 'minut', 'servic']]\n",
            "Starting training\n",
            "Fitting LDA ...\n",
            "Fitting LDA Done!\n",
            "Training on 58933 reviews took 0.5200243949890136 minutes\n",
            "[['order', 'time', 'food', 'servic', 'pizza'], ['cream', 'salad', 'pizza', 'la', 'sauc'], ['food', 'place', 'servic', 'bar', 'drink'], ['food', 'place', 'servic', 'restaur', 'beer'], ['pizza', 'place', 'crust', 'slice', 'sauc']]\n",
            "Starting training\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Finished getting vector representations for LDA\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:08<00:00, 48.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cf83a1927ce44eeb14ddf2286e747bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=7367.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished getting vector representations for BERT\n",
            "Fitting Autoencoder ...\n",
            "Fitting Autoencoder Done!\n",
            "Clustering embeddings. Done!\n",
            "Training on 58933 reviews took 25.045733698209126 minutes\n",
            "Topics are:\n",
            "[['food', 'place', 'servic', 'time', 'pizza'], ['pizza', 'place', 'order', 'food', 'time'], ['pizza', 'place', 'crust', 'time', 'sauc']]\n",
            "Topics are:\n",
            "[['food', 'place', 'servic', 'time', 'pizza'], ['pizza', 'place', 'order', 'food', 'time'], ['pizza', 'place', 'crust', 'time', 'sauc']]\n",
            "Starting training\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Finished getting vector representations for LDA\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a17d0962c1f4078b69e9276bf5b1ff7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=7367.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished getting vector representations for BERT\n",
            "Fitting Autoencoder ...\n",
            "Fitting Autoencoder Done!\n",
            "Clustering embeddings. Done!\n",
            "Training on 58933 reviews took 24.554742964108787 minutes\n",
            "Topics are:\n",
            "[['food', 'place', 'servic', 'time', 'restaur'], ['pizza', 'place', 'crust', 'sauc', 'time'], ['pizza', 'place', 'food', 'servic', 'time'], ['order', 'pizza', 'time', 'food', 'servic']]\n",
            "Topics are:\n",
            "[['food', 'place', 'servic', 'time', 'restaur'], ['pizza', 'place', 'crust', 'sauc', 'time'], ['pizza', 'place', 'food', 'servic', 'time'], ['order', 'pizza', 'time', 'food', 'servic']]\n",
            "Starting training\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Finished getting vector representations for LDA\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16ceaf540c724a6da71d65059aa4cdec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=7367.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished getting vector representations for BERT\n",
            "Fitting Autoencoder ...\n",
            "Fitting Autoencoder Done!\n",
            "Clustering embeddings. Done!\n",
            "Training on 58933 reviews took 25.028827889760336 minutes\n",
            "Topics are:\n",
            "[['pizza', 'place', 'food', 'time', 'servic'], ['pizza', 'food', 'place', 'sauc', 'time'], ['order', 'pizza', 'deliveri', 'time', 'minut'], ['pizza', 'place', 'crust', 'slice', 'sauc'], ['food', 'place', 'servic', 'time', 'pizza']]\n",
            "Topics are:\n",
            "[['pizza', 'place', 'food', 'time', 'servic'], ['pizza', 'food', 'place', 'sauc', 'time'], ['order', 'pizza', 'deliveri', 'time', 'minut'], ['pizza', 'place', 'crust', 'slice', 'sauc'], ['food', 'place', 'servic', 'time', 'pizza']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph1S4BbBbGEJ",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "We compare the hybrid topic cluster model with a pure LDA model. The models were run multiple times with a range of values for each hyperparameter.\n",
        "\n",
        "Observations:\n",
        "1. Coherence was consistently an order of magnitude higher for the pure LDA model. \n",
        "2. Silhoutte scores improved as the LDA vectors were weighted more heavily in the hybrid model\n",
        "3. The pure LDA model produced higher quality topics as judged by two human evaluators (us). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR5CIIYosI7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "036cd6e6-bc75-4557-b3f0-0071907f3045"
      },
      "source": [
        "stemmed_pizza_vocab = stem_words([\"crust\", \"slice\", \"sauce\", \"topping\", \"cheese\", \"tomato\", \"food\", \"pie\"])\n",
        "\n",
        "for method, output_dict in model_dict.items():\n",
        "  output_dict[\"scores\"] = []\n",
        "  topics = output_dict[\"topics\"]\n",
        "  for topic in topics:\n",
        "    score = 0\n",
        "    for i, word in reversed(list(enumerate(reversed(topic)))):\n",
        "      if word == \"pizza\": # most important word is worth double\n",
        "        score += (i + 1) * 2\n",
        "      elif word in stemmed_pizza_vocab:\n",
        "        score += (i + 1)\n",
        "    output_dict[\"scores\"].append(score)\n",
        "  print(\"method: {}, topic scores: {}\".format(method, model_dict[method][\"scores\"]))\n",
        "  print(\"method: {}, coherence: {}\".format(method, model_dict[method][\"coherence\"]))\n",
        "  if \"BERT\" in method:\n",
        "    print(\"method: {}, silhouette: {}\".format(method, model_dict[method][\"silhouette\"]))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method: LDA_3, topic scores: [5, 20, 11]\n",
            "method: LDA_3, coherence: 0.5756220333994923\n",
            "method: LDA_4, topic scores: [16, 5, 7, 8]\n",
            "method: LDA_4, coherence: 0.5606726787477581\n",
            "method: LDA_5, topic scores: [5, 7, 5, 5, 16]\n",
            "method: LDA_5, coherence: 0.5588625602705525\n",
            "method: LDA_BERT_3, topic scores: [7, 12, 14]\n",
            "method: LDA_BERT_3, coherence: 0.471871612979343\n",
            "method: LDA_BERT_3, silhouette: 0.4062187373638153\n",
            "method: LDA_BERT_4, topic scores: [5, 15, 13, 10]\n",
            "method: LDA_BERT_4, coherence: 0.49285342482084316\n",
            "method: LDA_BERT_4, silhouette: 0.31931912899017334\n",
            "method: LDA_BERT_5, topic scores: [13, 16, 8, 16, 7]\n",
            "method: LDA_BERT_5, coherence: 0.5077161744314702\n",
            "method: LDA_BERT_5, silhouette: 0.28183406591415405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_il3KDrXlSbT",
        "colab_type": "text"
      },
      "source": [
        "To filter reviews:\n",
        "1. Choose model with highest coherence\n",
        "2. From chosen model, choose topic with highest topic score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UASLssNyk1T2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_method = max(model_dict, key=lambda v: model_dict[v]['coherence'])\n",
        "best_topic = max(range(len(model_dict[best_method][\"scores\"])), key=model_dict[best_method][\"scores\"].__getitem__)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P02iiJwSQWYI",
        "colab_type": "text"
      },
      "source": [
        "For prediction with the pure LDA model, the topics are retrieved for a given review. If the pizza topic does not have a high score, our hypothesis is that discarding it will lead to improved performance for the image classifier.\n",
        "\n",
        "For prediction with the LDA+BERT version, a review is place into a cluster with the pretrained model. Similar to the pure model, reviews that are found to belong to a cluster (i.e topic) that doesn't put a strong emphasis on pizza should be discarded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZNfuv3QVgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "485db6fe52c541e78a154c5ee13554d5",
            "c113d259a92547ddb84052948b4dca4e",
            "bd9cbaf00df34a7f80753a80c3d4fa3f",
            "56dcc13f9c1b40e3b8d8b8f43670381b",
            "714cdfd9821f489aa24eb526ea6d9a76",
            "3845ad38b68344c48833c2fd5079d3c3",
            "13929f40712b419cb6eda17658317152",
            "e23ac6d22c0a467d948ecc00c7c55b68"
          ]
        },
        "outputId": "304d96ab-1f6b-47fd-c277-4d8303fa798d"
      },
      "source": [
        "pic_path = 'review_photos'\n",
        "pix_review_ids = [f for f in os.listdir(pic_path) if os.path.isdir(os.path.join(pic_path, f))]\n",
        "\n",
        "pic_review_df = text_df[text_df['review_id'].isin(pix_review_ids)]\n",
        "pic_review_df = pic_review_df.fillna('')\n",
        "reviews = pic_review_df.text\n",
        "reviews = reviews.reset_index(drop=True)\n",
        "\n",
        "#sentences, token_lists, idx_in = preprocess(reviews, len(reviews) + 1)\n",
        "processed_file = r'topic_data/processed_pics_real.pkl'\n",
        "with open(os.path.join(workdir, processed_file), 'rb') as p:\n",
        "    sentences, token_lists, idx_in = pickle.load(p)\n",
        "\n",
        "topic_list = model_dict[best_method][\"model\"].predict(sentences, token_lists, True)\n",
        "\n",
        "pic_review_df[\"main_topic\"] = topic_list\n",
        "food_based_reviews = pic_review_df[pic_review_df['main_topic']==best_topic]\n",
        "food_based_reviews = food_based_reviews.reset_index(drop=True)\n",
        "print(\"Some example reviews:\\n\")\n",
        "print(food_based_reviews.loc[0, \"text\"])\n",
        "print(\"---------------------------------------------------------------\")\n",
        "print(food_based_reviews.loc[1, \"text\"])\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting...\n",
            "Getting vector representations for LDA ...\n",
            "Finished getting vector representations for LDA\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "485db6fe52c541e78a154c5ee13554d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1288.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished getting vector representations for BERT\n",
            "[[0.5123532  0.         1.872729   ... 1.8326087  7.1875677  0.27425468]\n",
            " [4.095818   0.         3.472128   ... 2.5564518  3.6630652  1.7029277 ]\n",
            " [5.6492615  0.         2.9850845  ... 1.2736934  4.880928   2.0360363 ]\n",
            " ...\n",
            " [6.087328   0.         2.3836615  ... 1.4939708  5.699146   1.5553417 ]\n",
            " [9.971962   0.         2.9475718  ... 1.7674053  4.9357524  1.4321375 ]\n",
            " [6.865582   0.         2.6596682  ... 1.0894762  5.2388964  1.1467477 ]]\n",
            "[4 4 0 ... 3 2 0]\n",
            "Some example reviews:\n",
            "\n",
            "If you're in the area and have a hankering for pizza, head on over to the five50. Let's call this place a 3.5. Midwesterners beware, this is a thin crust pizza joint!\n",
            "\n",
            "We were pressed for time and had a 30 to 40 minute window for eating. The service was accommodating, and they got us in and out just in time. We ordered one pizza, half North Beach half Margherita. The slices were large and thin, so one pizza was enough to make two people full at three slices per person. The pizza was thin crust, but still had a slight chew to go with the crunch which I enjoyed. The North Beach had smoked mozzarella and clams on it, and was described as bbq chicken pizza with the chicken swapped for clams. It was unique with tender clam pieces, but I think they planted a dangerous seed since I found myself wishing it had a dash of bbq sauce! The Margherita pizza was nothing special, just a big basil leaf on each slice.\n",
            "---------------------------------------------------------------\n",
            "I heard this place was good and it was. I ordered a personal sized cheese pizza with no extra toppings for $7.50. and it was good. They make it like Pizza Brutta on Monroe St, where instead of being covered with a layer of cheese it has a few dollops. Novonta pulls that off (Pizza Brutta did too). Novonta's crust was good, and the rest of the pizza was even better. I really got just a plain cheese pizza, but there was more to it. Some sort of Basil or Spinach leaves in there... I don't know but it was good. It also only took like 2 minutes to make.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}