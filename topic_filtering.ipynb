{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_filtering.ipynb",
      "provenance": [],
      "mount_file_id": "16y--_ZDLfGLHuuQMJCVqYv8teqDHMxNC",
      "authorship_tag": "ABX9TyPE1+wLDKo+H5UCwC46Jr5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52768811a4f849d38629b3fee078f266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a0fa5f2e6cae474e9be838ddcf71c981",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8768dcee8ded4eb2853ce2921a61a7da",
              "IPY_MODEL_676996cc2bc345ee8f2c3ca9311ecdb4"
            ]
          }
        },
        "a0fa5f2e6cae474e9be838ddcf71c981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8768dcee8ded4eb2853ce2921a61a7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b8698c4d73ce45f8942e131dd747a62d",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db0281569be6477ea018e193db8f80f5"
          }
        },
        "676996cc2bc345ee8f2c3ca9311ecdb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1287c97a297f436ea51a614321498b22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [00:01&lt;00:00,  7.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e91283d4884c4d328ae1c8a93cb30f74"
          }
        },
        "b8698c4d73ce45f8942e131dd747a62d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db0281569be6477ea018e193db8f80f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1287c97a297f436ea51a614321498b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e91283d4884c4d328ae1c8a93cb30f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaifuss/data_science_seminar/blob/master/topic_filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vPFvziHYMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import datetime\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyS9X2QrgrTF",
        "colab_type": "text"
      },
      "source": [
        "The goal of this section is to identify distinct topics within the corpus of pizza review texts. Once topics are identified, reviews that do not contain food-related topics can be filtered out. This has the potential to improve the image classification in the next stage by weeding out irrelevent samples - where reviewers didn't base their scores on the pizza.\n",
        "\n",
        "This will be achieved by building a hybrid topic model that melds a latent Dirichlet allocation (LDA) model with a word vector clustering model.\n",
        "\n",
        "The premise of LDA (Biel et al., 2003) is that documents with similar topics use similar words. The algorithm aims to discover groups of words the occur frequently occur together in the same document. A topic is modeled as a probability distribution over words. Moreover, a document can be modeled as a probability distribution over different topics. \n",
        "\n",
        "Thus, the algorithm words as follows:\n",
        "\n",
        "\n",
        "1.   Remove unimportant words and set how many topics to find.\n",
        "2.   Randomly assign each word in each document to a random topic\n",
        "3.   For each document,\n",
        ">a. choose a topic, assuming all others are allocated correctly\n",
        "\n",
        ">>i. calculate the topic distribution within the document:  p(topic | document)\n",
        "\n",
        ">>ii. calculate the word distribution within the topic: p(word | topic)\n",
        "\n",
        ">> iii. multiply i and ii together and assign words to new topics based on the result\n",
        "\n",
        "4. terminate when there are no new assignments\n",
        "\n",
        "The model is finetuned by several parameters:\n",
        "Alpha reflects how many topics are in a given document (higher values lead to more topics per document in the model)\n",
        "Beta reflects how many words are in a given topic (higher values lead to more words per topic in the model).\n",
        "\n",
        " If a set of words are repeated in many documents, those words are said to be a topic.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfst91OkIcNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_reviews():\n",
        "  with open(r'/content/drive/My Drive/Data Science Class/pizza_reviews.json', 'r') as f:\n",
        "    pizza_reviews = json.load(f)\n",
        "  return pizza_reviews\n",
        "review_list = load_reviews()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVd2N2IXJAO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "64a1bb27-8749-4531-e36d-3014deca90aa"
      },
      "source": [
        "review_df = pd.DataFrame(review_list)\n",
        "\n",
        "text_df = review_df[['review_id', 'text']].copy()\n",
        "text_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mM8i91yWP1QbImEvz5ds0w</td>\n",
              "      <td>In the heart of Chinatown, I discovered it enr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>09qxjFi4abaW66JeSLazuQ</td>\n",
              "      <td>Was a Chicago style deep dish.  Homemade type ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>K-wdPGHbErfxbKK6PetrmA</td>\n",
              "      <td>First time eating there and everything was so ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jkVxX4ieJwVRO9n4E8tNMw</td>\n",
              "      <td>More than just  Pizza. This location is small ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lb9r62Qlu12ZB909CbFeOQ</td>\n",
              "      <td>I ordered a pizza at 4:49. Got an email that s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id                                               text\n",
              "0  mM8i91yWP1QbImEvz5ds0w  In the heart of Chinatown, I discovered it enr...\n",
              "1  09qxjFi4abaW66JeSLazuQ  Was a Chicago style deep dish.  Homemade type ...\n",
              "2  K-wdPGHbErfxbKK6PetrmA  First time eating there and everything was so ...\n",
              "3  jkVxX4ieJwVRO9n4E8tNMw  More than just  Pizza. This location is small ...\n",
              "4  Lb9r62Qlu12ZB909CbFeOQ  I ordered a pizza at 4:49. Got an email that s..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUQlNqBINkOG",
        "colab_type": "text"
      },
      "source": [
        "word level preprocessing utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUvCif1fM3Py",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "f734cdef-fdd0-4f67-e717-073fe437a257"
      },
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/47/0ed64014af68aaf36f2e0a42bb30a5caf82e54edf92329d8aca4959ba9d7/sentence-transformers-0.2.6.2.tar.gz (60kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 1.8MB/s \n",
            "\u001b[?25hCollecting transformers==2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->sentence-transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->sentence-transformers) (20.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 19.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0->sentence-transformers) (2.9)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.2-cp36-none-any.whl size=79982 sha256=fac69e30e47c39bb4b3af730f33463533e3388dae1a651e4282d10eb5c6cfb8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/fc/be/8482e9a1e1313463bba5c9749e34657017afc251919d909321\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=07c31fbb30e97abd2fca8be8da03732adf4c154b1e83b1cdd1a2b60ce1172852\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.2 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkiPLh-NzPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "94f1e11b-4560-4f2f-d765-5d2afecaff25"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f69KGrPSNRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regex_filter(sentence):\n",
        "    # fix missing delimiter - i.e deepDishPizza\n",
        "    sentence = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'&gt|&lt', ' ', sentence)\n",
        "    # fix letter repetition (if more than 2)\n",
        "    sentence = re.sub(r'([a-z])\\1{2,}', r'\\1', sentence)\n",
        "    # fix non-word repetition (if more than 1)\n",
        "    sentence = re.sub(r'([\\W+])\\1{1,}', r'\\1', sentence)\n",
        "    # string * as delimiter\n",
        "    sentence = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', sentence)\n",
        "    # xxx[?!]. -- > xxx.\n",
        "    sentence = re.sub(r'\\W+?\\.', '.', sentence)\n",
        "    # [.?!] --> [.?!] xxx\n",
        "    sentence = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', sentence)\n",
        "    # fix phrase repetition\n",
        "    sentence = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', sentence)\n",
        "\n",
        "    return sentence.strip()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bljAxTQuNLZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove numbers and punctuation marks\n",
        "def filter_punctuation(word_list):\n",
        "    return [word for word in word_list if word.isalpha()]\n",
        "\n",
        "# remove unimportant connective words such as \"and\", \"the\", etc\n",
        "def filter_stopwords(word_list):\n",
        "  return [word for word in word_list if word not in stopwords.words('english')]\n",
        "\n",
        "# TODO is this helpful or not?\n",
        "def retain_nouns(word_list):\n",
        "    return [word for (word, pos) in nltk.pos_tag(word_list) if pos[:2] in ['NN']] #, 'VBP', 'VBN']]\n",
        "\n",
        "# normlize for part of speech\n",
        "def stem_words(word_list):\n",
        "  ps = PorterStemmer()\n",
        "  return [ps.stem(word) for word in word_list]\n",
        "\n",
        "def fix_spelling(word_list):\n",
        "  spell = SpellChecker()\n",
        "  return [spell.correction(word) for word in word_list]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23SUE1RhVN83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_words(text):\n",
        "  word_list = word_tokenize(text)\n",
        "  word_list = filter_punctuation(word_list)\n",
        "  word_list = fix_spelling(word_list) \n",
        "  word_list = filter_stopwords(word_list)\n",
        "  word_list = retain_nouns(word_list)\n",
        "  return stem_words(word_list)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmw52otQ4cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(reviews, samp_size=None):\n",
        "  if not samp_size:\n",
        "        samp_size = 1000\n",
        "\n",
        "  start = time.time()\n",
        "  print('Stage 1: Preprocess raw review texts')\n",
        "  #review_count = len(reviews)\n",
        "  texts = []  \n",
        "  token_lists = []  \n",
        "  idx_in = []\n",
        "\n",
        "  indicies = np.random.choice(len(reviews), samp_size)\n",
        "  for i in indicies:\n",
        "      text = regex_filter(reviews[i])\n",
        "      token_list = preprocess_words(text)\n",
        "      if token_list:\n",
        "          idx_in.append(i)\n",
        "          texts.append(text)\n",
        "          token_lists.append(token_list)\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Preprocessing {} reviews took {} minutes\".format(len(indicies), str((end - start)/60)))\n",
        "  return texts, token_lists, idx_in\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYa3ohLeZHkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df4c0945-82e7-425f-92c7-8357f7b41cad"
      },
      "source": [
        "sentences, token_lists, idx_in = pre_process(text_df.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stage 1: Preprocess raw review texts\n",
            "Preprocessing 479792 reviews took 9.448280354340872 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_Yl6tIaO2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fbb1117-2298-43ca-953b-fef55d501478"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=200,\n",
        "                                        batch_size=128,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIiZR7zi4rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, k=5, method='LDA_BERT'):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 15  # parameter for reletive importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + str(time.time())\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        Get vector representations from selected methods\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'LDA':\n",
        "            print('Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20)\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            print('Finished getting vector representations for LDA')\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            print('Finished getting vector representations for BERT')\n",
        "            return vec\n",
        "\n",
        "        #         elif method == 'LDA_BERT':\n",
        "        else: \n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('Fitting Autoencoder ...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "                print('Fitting Autoencoder Done!')\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        Fit the topic model for selected method given the preprocessed data\n",
        "        :docs: list of documents, each doc is preprocessed as tokens\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('Fitting LDA ...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20)\n",
        "                print('Fitting LDA Done!')\n",
        "        else:\n",
        "            print('Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "            print('Clustering embeddings. Done!')\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "        \"\"\"\n",
        "        Predict topics for new_documents\n",
        "        \"\"\"\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":\n",
        "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
        "                                    corpus)))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66rFLjZscSGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2urPDtaoRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outfile = r'/content/drive/My Drive/Data Science Class/tm.file'\n",
        "\n",
        "def main():\n",
        "    \n",
        "  method = \"LDA_BERT\"\n",
        "  samp_size = 100\n",
        "  ntopic = 5\n",
        "\n",
        "  data = text_df\n",
        "  data = data.fillna('')  # only the comments has NaN's\n",
        "  reviews = data.text\n",
        "  sentences, token_lists, idx_in = preprocess(reviews, samp_size=samp_size)\n",
        "  tm = Topic_Model(k = ntopic)\n",
        "  tm.fit(sentences, token_lists)\n",
        "  with open(outfile, 'wb') as f:\n",
        "    pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n",
        "  \n",
        "  # coherence measures internal consistency of a topic\n",
        "  #print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
        "  # silhoutte measures consistency of clusters\n",
        "  #print('Silhouette Score:', get_silhouette(tm))\n",
        "  # visualize and save img\n",
        "  #visualize(tm)\n",
        "  #for i in range(tm.k):\n",
        "  #  get_wordcloud(tm, token_lists, i)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WW2YNPum1vS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518,
          "referenced_widgets": [
            "52768811a4f849d38629b3fee078f266",
            "a0fa5f2e6cae474e9be838ddcf71c981",
            "8768dcee8ded4eb2853ce2921a61a7da",
            "676996cc2bc345ee8f2c3ca9311ecdb4",
            "b8698c4d73ce45f8942e131dd747a62d",
            "db0281569be6477ea018e193db8f80f5",
            "1287c97a297f436ea51a614321498b22",
            "e91283d4884c4d328ae1c8a93cb30f74"
          ]
        },
        "outputId": "fd465a8c-b5e0-45d9-a7c4-8ea38722779e"
      },
      "source": [
        "main()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stage 1: Preprocess raw review texts\n",
            "Preprocessing 100 reviews took 1.1484277844429016 minutes\n",
            "Clustering embeddings ...\n",
            "Getting vector representations for LDA ...\n",
            "Finished getting vector representations for LDA\n",
            "Getting vector representations for BERT ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405M/405M [00:44<00:00, 9.20MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52768811a4f849d38629b3fee078f266",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=13.0, style=ProgressStyle(description_widthâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished getting vector representations for BERT\n",
            "Fitting Autoencoder ...\n",
            "Fitting Autoencoder Done!\n",
            "Clustering embeddings. Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-603b2b415544>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# coherence measures internal consistency of a topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Coherence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;31m# silhoutte measures consistency of clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Silhouette Score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_silhouette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_coherence' is not defined"
          ]
        }
      ]
    }
  ]
}