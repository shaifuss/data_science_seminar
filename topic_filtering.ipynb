{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_filtering.ipynb",
      "provenance": [],
      "mount_file_id": "16y--_ZDLfGLHuuQMJCVqYv8teqDHMxNC",
      "authorship_tag": "ABX9TyMaTAgE7cv8aH7J2mnIOqwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaifuss/data_science_seminar/blob/master/topic_filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vPFvziHYMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "!pip install pyspellchecker\n",
        "!pip install sentence-transformers\n",
        "\n",
        "workdir = r'/content/drive/My Drive/Data Science Class'\n",
        "tmfile = r'tm.file'\n",
        "processed_file = r'topic_data/processed_max.pkl'\n",
        "PERSIST_INTERVAL = 30 # in minutes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyS9X2QrgrTF",
        "colab_type": "text"
      },
      "source": [
        "In this section we aim to identify distinct topics discussed in the corpus of pizza review texts. Once topics are identified, reviews that do not contain food-related topics can be filtered out. This has the potential to improve the image classification in the next stage by weeding out irrelevent samples - where reviewers didn't base their scores on the pizza.\n",
        "\n",
        "This will be achieved by building a hybrid topic model that melds a latent Dirichlet allocation (LDA) model with a word vector clustering model.\n",
        "\n",
        "The premise of LDA (Biel et al., 2003) is that documents with similar topics use similar words. The algorithm aims to discover groups of words the occur frequently occur together in the same document. A topic is modeled as a probability distribution over words. Moreover, a document can be modeled as a probability distribution over different topics. \n",
        "\n",
        "Thus, the algorithm words as follows:\n",
        "\n",
        "\n",
        "1.   Remove unimportant words and set how many topics to find.\n",
        "2.   Randomly assign each word in each document to a random topic\n",
        "3.   For each document,\n",
        ">a. choose a topic, assuming all others are allocated correctly\n",
        "\n",
        ">>i. calculate the topic distribution within the document:  p(topic | document)\n",
        "\n",
        ">>ii. calculate the word distribution within the topic: p(word | topic)\n",
        "\n",
        ">> iii. multiply i and ii together and assign words to new topics based on the result\n",
        "\n",
        "4. terminate when there are no new assignments\n",
        "\n",
        "The model is finetuned by several parameters:\n",
        "Alpha reflects how many topics are in a given document (higher values lead to more topics per document in the model)\n",
        "Beta reflects how many words are in a given topic (higher values lead to more words per topic in the model). In this implementation, the model is set to learn these values automatically\n",
        "\n",
        "Words clusters are crafted by first calculating texts into vector representations. A clustering algorithm is then performed to find clusters of text vectors in high dimension vector space. \n",
        "\n",
        "This model utilizes BERT (Devlin et al., 2018) to craft word embeddings that are concatenated into text vectors. BERT is a pretrained language model from researchers at Google that utilizes a multiheaded transformer ANN architecture to learn the meaning of words from the context in which they are found. BERT and its successors have acheived impressive performance on language understatnding tasks like question answering and others. \n",
        "\n",
        "The LDA and BERT outputs are concatenated together and clustering is performed (the default algorithm used here is KMeans). \n",
        "\n",
        "In an attempt to improve the clustering, dimensionality reduction is attempted. The  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfst91OkIcNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7e5e9bc3-d1d2-4764-b4b2-f63b61e062bb"
      },
      "source": [
        "# loading reviews into dataframe and peak at the data\n",
        "def load_reviews():\n",
        "  with open(r'/content/drive/My Drive/Data Science Class/pizza_reviews.json', 'r') as f:\n",
        "    pizza_reviews = json.load(f)\n",
        "  return pizza_reviews\n",
        "review_list = load_reviews()\n",
        "print(len(review_list))\n",
        "review_df = pd.DataFrame(review_list)\n",
        "\n",
        "text_df = review_df[['review_id', 'text']].copy()\n",
        "text_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "479792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mM8i91yWP1QbImEvz5ds0w</td>\n",
              "      <td>In the heart of Chinatown, I discovered it enr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>09qxjFi4abaW66JeSLazuQ</td>\n",
              "      <td>Was a Chicago style deep dish.  Homemade type ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>K-wdPGHbErfxbKK6PetrmA</td>\n",
              "      <td>First time eating there and everything was so ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jkVxX4ieJwVRO9n4E8tNMw</td>\n",
              "      <td>More than just  Pizza. This location is small ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lb9r62Qlu12ZB909CbFeOQ</td>\n",
              "      <td>I ordered a pizza at 4:49. Got an email that s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id                                               text\n",
              "0  mM8i91yWP1QbImEvz5ds0w  In the heart of Chinatown, I discovered it enr...\n",
              "1  09qxjFi4abaW66JeSLazuQ  Was a Chicago style deep dish.  Homemade type ...\n",
              "2  K-wdPGHbErfxbKK6PetrmA  First time eating there and everything was so ...\n",
              "3  jkVxX4ieJwVRO9n4E8tNMw  More than just  Pizza. This location is small ...\n",
              "4  Lb9r62Qlu12ZB909CbFeOQ  I ordered a pizza at 4:49. Got an email that s..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di_BNmLcN5Bv",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is necessary to create a universal vocabulary for the corpus. Each text is first processed at the string level and then at the word level. \n",
        "\n",
        "Stages:\n",
        "1. fix typos and missing spaces\n",
        "2. remove punctuation, capitalization, and numbers\n",
        "3. remove unimportant words (stopwords)\n",
        "4. stem words so that a fair comparison can be made (for example, salted and salty both become salt), "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkiPLh-NzPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "66380de2-d1bc-4dd6-bc48-ff7466aa4d92"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f69KGrPSNRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regex_filter(sentence):\n",
        "    # fix missing delimiter - i.e deepDishPizza\n",
        "    sentence = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'&gt|&lt', ' ', sentence)\n",
        "    # fix letter repetition (if more than 2)\n",
        "    sentence = re.sub(r'([a-z])\\1{2,}', r'\\1', sentence)\n",
        "    # fix non-word repetition (if more than 1)\n",
        "    sentence = re.sub(r'([\\W+])\\1{1,}', r'\\1', sentence)\n",
        "    # string * as delimiter\n",
        "    sentence = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', sentence)\n",
        "    # xxx[?!]. -- > xxx.\n",
        "    sentence = re.sub(r'\\W+?\\.', '.', sentence)\n",
        "    # [.?!] --> [.?!] xxx\n",
        "    sentence = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', sentence)\n",
        "    # fix phrase repetition\n",
        "    sentence = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', sentence)\n",
        "\n",
        "    return sentence.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bljAxTQuNLZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove numbers and punctuation marks\n",
        "def filter_punctuation(word_list):\n",
        "    return [word for word in word_list if word.isalpha()]\n",
        "\n",
        "# remove unimportant connective words such as \"and\", \"the\", etc\n",
        "def filter_stopwords(word_list):\n",
        "  return [word for word in word_list if word not in stopwords.words('english')]\n",
        "\n",
        "# keep only nouns\n",
        "def retain_nouns(word_list):\n",
        "    return [word for (word, pos) in nltk.pos_tag(word_list) if pos[:2] in ['NN']] #, 'VBP', 'VBN']]\n",
        "\n",
        "# normlize for part of speech\n",
        "def stem_words(word_list):\n",
        "  ps = PorterStemmer()\n",
        "  return [ps.stem(word) for word in word_list]\n",
        "\n",
        "def fix_spelling(word_list):\n",
        "  spell = SpellChecker()\n",
        "  return [spell.correction(word) for word in word_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23SUE1RhVN83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_words(text):\n",
        "  word_list = word_tokenize(text)\n",
        "  word_list = filter_punctuation(word_list)\n",
        "  word_list = fix_spelling(word_list) \n",
        "  word_list = filter_stopwords(word_list)\n",
        "  word_list = retain_nouns(word_list)\n",
        "  return stem_words(word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94WRVmMGhyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def persist_processed_data(texts, token_lists, idx_in):\n",
        "  print(\"Saving at {}\".format(time.time()))\n",
        "  with open(os.path.join(workdir, processed_file), 'wb') as p:\n",
        "          pickle.dump([texts, token_lists, idx_in], p)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmw52otQ4cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(reviews, samp_size=None):\n",
        "  if not samp_size:\n",
        "        samp_size = 1000\n",
        "\n",
        "  start = time.time()\n",
        "  print('Stage 1: Preprocess raw review texts')\n",
        "  texts = []  \n",
        "  token_lists = []  \n",
        "  idx_in = []\n",
        "  batch_start = time.time()\n",
        "  indicies = np.random.choice(len(reviews), samp_size)\n",
        "  for i in indicies:\n",
        "      text = regex_filter(reviews[i])\n",
        "      token_list = preprocess_words(text)\n",
        "      if token_list:\n",
        "        idx_in.append(i)\n",
        "        texts.append(text)\n",
        "        token_lists.append(token_list)\n",
        "      batch_end = time.time() \n",
        "      if ((batch_end - batch_start)/60) > PERSIST_INTERVAL:\n",
        "        batch_start = batch_end\n",
        "        persist_processed_data(texts, token_lists, idx_in)\n",
        "           \n",
        "  end = time.time()\n",
        "  print(\"Preprocessing {} reviews took {} minutes\".format(len(indicies), str((end - start)/60)))\n",
        "  persist_processed_data(texts, token_lists, idx_in)\n",
        "  return texts, token_lists, idx_in\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXvFzyNtN0gr",
        "colab_type": "text"
      },
      "source": [
        "talk about model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_Yl6tIaO2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64287fd4-e8c3-47be-c5a6-a6076ca29d55"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=200,\n",
        "                                        batch_size=128,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIiZR7zi4rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, k=4, method='LDA_BERT'):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 15  # parameter for reletive importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + str(time.time())\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        Get vector representations from selected methods\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'LDA':\n",
        "            print('Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20, alpha='auto', )\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            print('Finished getting vector representations for LDA')\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            print('Finished getting vector representations for BERT')\n",
        "            return vec\n",
        "\n",
        "        #         elif method == 'LDA_BERT':\n",
        "        else: \n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('Fitting Autoencoder ...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "                print('Fitting Autoencoder Done!')\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        Fit the topic model for selected method given the preprocessed data\n",
        "        :docs: list of documents, each doc is preprocessed as tokens\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('Fitting LDA ...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary\n",
        "                                                                , alpha='auto', eta='auto', minimum_probability=0.3)\n",
        "                print('Fitting LDA Done!')\n",
        "        else:\n",
        "            print('Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "            print('Clustering embeddings. Done!')\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "        \"\"\"\n",
        "        Predict topics for new_documents\n",
        "        \"\"\"\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":\n",
        "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
        "                                    corpus)))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVWTU8iNQR01",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsan4epLeHG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.metrics import silhouette_score\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import os\n",
        "\n",
        "\n",
        "def get_topic_words(token_lists, labels, k=None):\n",
        "    \"\"\"\n",
        "    get top words within each topic from clustering results\n",
        "    \"\"\"\n",
        "    if k is None:\n",
        "        k = len(np.unique(labels))\n",
        "    topics = ['' for _ in range(k)]\n",
        "    for i, c in enumerate(token_lists):\n",
        "        topics[labels[i]] += (' ' + ' '.join(c))\n",
        "    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
        "    # get sorted word counts\n",
        "    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
        "    # get topics\n",
        "    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n",
        "    print(\"Topics are:\\n{}\".format(topics))\n",
        "    return topics\n",
        "\n",
        "def get_coherence(model, token_lists, measure='c_v'):\n",
        "    \"\"\"\n",
        "    Get model coherence from gensim.models.coherencemodel\n",
        "    :param model: Topic_Model object\n",
        "    :param token_lists: token lists of docs\n",
        "    :param topics: topics as top words\n",
        "    :param measure: coherence metrics\n",
        "    :return: coherence score\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
        "                            coherence=measure)\n",
        "    else:\n",
        "        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
        "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
        "                            coherence=measure)\n",
        "    return cm.get_coherence()\n",
        "\n",
        "def get_silhouette(model):\n",
        "    \"\"\"\n",
        "    Get silhouette score from model\n",
        "    :param model: Topic_Model object\n",
        "    :return: silhouette score\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        return\n",
        "    lbs = model.cluster_model.labels_\n",
        "    vec = model.vec[model.method]\n",
        "    return silhouette_score(vec, lbs)\n",
        "\n",
        "def plot_proj(embedding, lbs):\n",
        "    \"\"\"\n",
        "    Plot UMAP embeddings\n",
        "    :param embedding: UMAP (or other) embeddings\n",
        "    :param lbs: labels\n",
        "    \"\"\"\n",
        "    n = len(embedding)\n",
        "    counter = Counter(lbs)\n",
        "    for i in range(len(np.unique(lbs))):\n",
        "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
        "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YNL6n9hkNoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outdir = r'/content/drive/My Drive/Data Science Class/viz'\n",
        "\n",
        "def visualize(model):\n",
        "    \"\"\"\n",
        "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
        "    :param model: Topic_Model object\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        return\n",
        "    reducer = umap.UMAP()\n",
        "    print('Calculating UMAP projection ...')\n",
        "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
        "    print('Calculating UMAP projection. Done!')\n",
        "    plot_proj(vec_umap, model.cluster_model.labels_)\n",
        "    #dr = outdir + '_{}_{}.jpeg'.format(model.method, model.id)    # '/kaggle/working/contextual_topic_identification/docs/images/{}/{}'.format(model.method, model.id)\n",
        "    #if not os.path.exists(dr):\n",
        "    #    os.makedirs(dr)\n",
        "    plt.savefig(outdir + '/{}_{}.jpeg'.format(model.method, model.id), format='jpeg')      #'/kaggle/working/2D_vis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXOw_qL4_O0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    \n",
        "  method = \"LDA_BERT\"\n",
        "  samp_size = 1000\n",
        "  ntopic = 4\n",
        "\n",
        "  data = text_df\n",
        "  data = data.fillna('')  # only the comments has NaN's\n",
        "  reviews = data.text\n",
        "  sentences, token_lists, idx_in = preprocess(reviews, samp_size=samp_size)\n",
        "  #with open(os.path.join(workdir, processed_file), 'wb') as p:\n",
        "  #  pickle.dump([sentences, token_lists, idx_in], p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDekpuykvS47",
        "colab_type": "text"
      },
      "source": [
        "The number of topics is set to 4, reflecting an assumption of the following topics:\n",
        "\n",
        "1. Food\n",
        "2. Service\n",
        "3. Atmosphere\n",
        "4. Value\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY2urPDtaoRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train():\n",
        "    \n",
        "  processed_file = r'topic_data/processed_58933.pkl'\n",
        "  method = \"LDA_BERT\"\n",
        "  #samp_size = 500\n",
        "  ntopic = 3\n",
        "\n",
        "  #data = text_df\n",
        "  #data = data.fillna('')  # only the comments has NaN's\n",
        "  #reviews = data.text\n",
        "  #sentences, token_lists, idx_in = preprocess(reviews, samp_size=samp_size)\n",
        "  \n",
        "  with open(os.path.join(workdir, processed_file), 'rb') as p:\n",
        "    sentences, token_lists, idx_in = pickle.load(p)\n",
        "\n",
        "  print(\"Starting training\")\n",
        "  start = time.time()\n",
        "  tm = Topic_Model(k = ntopic)\n",
        "  tm.fit(sentences, token_lists)\n",
        "  end = time.time()\n",
        "  print(\"Training on {} reviews took {} minutes\".format(len(sentences), str((end - start)/60)))\n",
        "  with open(os.path.join(workdir, tmfile), 'wb') as f:\n",
        "    pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n",
        "  \n",
        "  # coherence measures internal consistency of a topic\n",
        "  print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
        "  # silhoutte measures consistency of clusters\n",
        "  print('Silhouette Score:', get_silhouette(tm))\n",
        "  # visualize and save img\n",
        "  visualize(tm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQQy8uEoa3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WW2YNPum1vS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fe44b78f-2bc8-44d1-fe12-3d6d713b15d1"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stage 1: Preprocess raw review texts\n",
            "Preprocessing 1000 reviews took 11.134337544441223 minutes\n",
            "Saving at 1594360174.5739603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P02iiJwSQWYI",
        "colab_type": "text"
      },
      "source": [
        "talk about metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZNfuv3QVgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43e5dc5f-7302-46aa-e0d4-79e9a249dd84"
      },
      "source": [
        "import os\n",
        "with open(os.path.join(workdir, r'topic_data/processed_200000.pkl'), 'rb') as p:\n",
        "    sentences, token_lists, idx_in = pickle.load(p)\n",
        "\n",
        "print(len(idx_in))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ndAxOwQluC",
        "colab_type": "text"
      },
      "source": [
        "visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0CuXkRkQl5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize(tm)\n",
        "  #for i in range(tm.k):\n",
        "  #  get_wordcloud(tm, token_lists, i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}